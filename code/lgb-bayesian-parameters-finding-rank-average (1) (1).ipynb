{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67f1ef572ea1f522bcdc6a775bf629ee740d87ac"
   },
   "source": [
    "# Bayesian global optimization with gaussian processes for finding (sub-)optimal parameters of LightGBM\n",
    "\n",
    "As many of fellow kaggler asking how did I get LightGBM parameters for the kernel [Customer Transaction Prediction](https://www.kaggle.com/fayzur/customer-transaction-prediction) I published. So, I decided to publish a kernel to optimize parameters. \n",
    "\n",
    "\n",
    "\n",
    "In this kernel I use Bayesian global optimization with gaussian processes for finding optimal parameters. This optimization attempts to find the maximum value of an black box function in as few iterations as possible. In our case the black box function will be a function that I will write to optimize (maximize) the evaluation function (AUC) so that parameters get maximize AUC in training and validation, and expect to do good in the private. The final prediction will be **rank average on 5 fold cross validation predictions**.\n",
    "\n",
    "Continue to the end of this kernel and **upvote it if you find it is interesting**.\n",
    "\n",
    "![image.jpg](https://i.imgur.com/XKS1oqU.jpg)\n",
    "\n",
    "Image taken from : https://github.com/fmfn/BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7409352edc21584713b1225028f087c6989e94be"
   },
   "source": [
    "## Notebook  Content\n",
    "0. [Installing Bayesian global optimization library](#0) <br>    \n",
    "1. [Loading the data](#1)\n",
    "2. [Black box function to be optimized (LightGBM)](#2)\n",
    "3. [Training LightGBM model](#3)\n",
    "4. [Rank averaging](#4)\n",
    "5. [Submission](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea005def562ae65202ec9322bec60fd25a1961e1"
   },
   "source": [
    "<a id=\"0\"></a> <br>\n",
    "## 0. Installing Bayesian global optimization library\n",
    "\n",
    "Let's install the latest release from pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "81c2a7386319cae39c1cf83d39394d530f549128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in /opt/conda/lib/python3.6/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from bayesian-optimization) (1.1.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /opt/conda/lib/python3.6/site-packages (from bayesian-optimization) (0.20.2)\r\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from bayesian-optimization) (1.16.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea005def562ae65202ec9322bec60fd25a1961e1"
   },
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import rankdata\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "f5624fd428106888ec023312d3479d324ef0eac9"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f1=open(r'F:\\zxd\\Kaggle\\santander-customer-transaction-prediction\\Submit\\train.txt','rb')\n",
    "train_df=pickle.load(f1)\n",
    "f1.close()\n",
    "f=open(r'F:\\zxd\\Kaggle\\santander-customer-transaction-prediction\\Submit\\test.txt','rb')\n",
    "test_df=pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "We are given anonymized dataset containing 200 numeric feature variables from var_0 to var_199. Let's have a look train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "7365ac9a050f611cb284bbb47519e04ac1ee19f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>...</th>\n",
       "      <th>var_196_ratio</th>\n",
       "      <th>var_197_ratio</th>\n",
       "      <th>var_198_ratio</th>\n",
       "      <th>var_2_ratio</th>\n",
       "      <th>var_22_ratio</th>\n",
       "      <th>var_24_ratio</th>\n",
       "      <th>var_25_ratio</th>\n",
       "      <th>var_26_ratio</th>\n",
       "      <th>var_28_ratio</th>\n",
       "      <th>var_3_ratio</th>\n",
       "      <th>var_32_ratio</th>\n",
       "      <th>var_34_ratio</th>\n",
       "      <th>var_35_ratio</th>\n",
       "      <th>var_36_ratio</th>\n",
       "      <th>var_37_ratio</th>\n",
       "      <th>var_39_ratio</th>\n",
       "      <th>var_4_ratio</th>\n",
       "      <th>var_40_ratio</th>\n",
       "      <th>var_42_ratio</th>\n",
       "      <th>var_43_ratio</th>\n",
       "      <th>var_44_ratio</th>\n",
       "      <th>var_48_ratio</th>\n",
       "      <th>var_49_ratio</th>\n",
       "      <th>var_5_ratio</th>\n",
       "      <th>var_50_ratio</th>\n",
       "      <th>var_51_ratio</th>\n",
       "      <th>var_52_ratio</th>\n",
       "      <th>var_53_ratio</th>\n",
       "      <th>var_55_ratio</th>\n",
       "      <th>var_56_ratio</th>\n",
       "      <th>var_59_ratio</th>\n",
       "      <th>var_6_ratio</th>\n",
       "      <th>var_60_ratio</th>\n",
       "      <th>var_61_ratio</th>\n",
       "      <th>var_62_ratio</th>\n",
       "      <th>var_63_ratio</th>\n",
       "      <th>var_64_ratio</th>\n",
       "      <th>var_66_ratio</th>\n",
       "      <th>var_67_ratio</th>\n",
       "      <th>var_68_ratio</th>\n",
       "      <th>var_69_ratio</th>\n",
       "      <th>var_70_ratio</th>\n",
       "      <th>var_71_ratio</th>\n",
       "      <th>var_72_ratio</th>\n",
       "      <th>var_73_ratio</th>\n",
       "      <th>var_74_ratio</th>\n",
       "      <th>var_75_ratio</th>\n",
       "      <th>var_76_ratio</th>\n",
       "      <th>var_78_ratio</th>\n",
       "      <th>var_79_ratio</th>\n",
       "      <th>var_80_ratio</th>\n",
       "      <th>var_81_ratio</th>\n",
       "      <th>var_82_ratio</th>\n",
       "      <th>var_83_ratio</th>\n",
       "      <th>var_84_ratio</th>\n",
       "      <th>var_85_ratio</th>\n",
       "      <th>var_86_ratio</th>\n",
       "      <th>var_88_ratio</th>\n",
       "      <th>var_89_ratio</th>\n",
       "      <th>var_9_ratio</th>\n",
       "      <th>var_90_ratio</th>\n",
       "      <th>var_91_ratio</th>\n",
       "      <th>var_92_ratio</th>\n",
       "      <th>var_93_ratio</th>\n",
       "      <th>var_94_ratio</th>\n",
       "      <th>var_95_ratio</th>\n",
       "      <th>var_96_ratio</th>\n",
       "      <th>var_97_ratio</th>\n",
       "      <th>var_99_ratio</th>\n",
       "      <th>var_104_ratio</th>\n",
       "      <th>var_107_ratio</th>\n",
       "      <th>var_112_ratio</th>\n",
       "      <th>var_121_ratio</th>\n",
       "      <th>var_14_ratio</th>\n",
       "      <th>var_140_ratio</th>\n",
       "      <th>var_142_ratio</th>\n",
       "      <th>var_149_ratio</th>\n",
       "      <th>var_156_ratio</th>\n",
       "      <th>var_160_ratio</th>\n",
       "      <th>var_172_ratio</th>\n",
       "      <th>var_177_ratio</th>\n",
       "      <th>var_178_ratio</th>\n",
       "      <th>var_186_ratio</th>\n",
       "      <th>var_192_ratio</th>\n",
       "      <th>var_199_ratio</th>\n",
       "      <th>var_20_ratio</th>\n",
       "      <th>var_21_ratio</th>\n",
       "      <th>var_23_ratio</th>\n",
       "      <th>var_31_ratio</th>\n",
       "      <th>var_33_ratio</th>\n",
       "      <th>var_45_ratio</th>\n",
       "      <th>var_57_ratio</th>\n",
       "      <th>var_65_ratio</th>\n",
       "      <th>var_77_ratio</th>\n",
       "      <th>var_8_ratio</th>\n",
       "      <th>var_87_ratio</th>\n",
       "      <th>var_120_ratio</th>\n",
       "      <th>var_46_ratio</th>\n",
       "      <th>var_54_ratio</th>\n",
       "      <th>var_58_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>2.9252</td>\n",
       "      <td>3.1821</td>\n",
       "      <td>14.0137</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>8.7989</td>\n",
       "      <td>14.5691</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>-7.2393</td>\n",
       "      <td>4.2840</td>\n",
       "      <td>30.7133</td>\n",
       "      <td>10.5350</td>\n",
       "      <td>16.2191</td>\n",
       "      <td>2.5791</td>\n",
       "      <td>2.4716</td>\n",
       "      <td>14.3831</td>\n",
       "      <td>13.4325</td>\n",
       "      <td>-5.1488</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>4.9306</td>\n",
       "      <td>5.9965</td>\n",
       "      <td>-0.3085</td>\n",
       "      <td>12.9041</td>\n",
       "      <td>-3.8766</td>\n",
       "      <td>16.8911</td>\n",
       "      <td>11.1920</td>\n",
       "      <td>10.5785</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>7.8871</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>3.8743</td>\n",
       "      <td>-5.2387</td>\n",
       "      <td>7.3746</td>\n",
       "      <td>11.5767</td>\n",
       "      <td>12.0446</td>\n",
       "      <td>11.6418</td>\n",
       "      <td>-7.0170</td>\n",
       "      <td>5.9226</td>\n",
       "      <td>-14.2136</td>\n",
       "      <td>16.0283</td>\n",
       "      <td>5.3253</td>\n",
       "      <td>12.9194</td>\n",
       "      <td>29.0460</td>\n",
       "      <td>-0.6940</td>\n",
       "      <td>5.1736</td>\n",
       "      <td>-0.7474</td>\n",
       "      <td>14.8322</td>\n",
       "      <td>11.2668</td>\n",
       "      <td>5.3822</td>\n",
       "      <td>2.0183</td>\n",
       "      <td>10.1166</td>\n",
       "      <td>16.1828</td>\n",
       "      <td>4.9590</td>\n",
       "      <td>2.0771</td>\n",
       "      <td>-0.2154</td>\n",
       "      <td>8.6748</td>\n",
       "      <td>9.5319</td>\n",
       "      <td>5.8056</td>\n",
       "      <td>22.4321</td>\n",
       "      <td>5.0109</td>\n",
       "      <td>-4.7010</td>\n",
       "      <td>21.6374</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>5.1999</td>\n",
       "      <td>8.8600</td>\n",
       "      <td>43.1127</td>\n",
       "      <td>18.3816</td>\n",
       "      <td>-2.3440</td>\n",
       "      <td>23.4104</td>\n",
       "      <td>6.5199</td>\n",
       "      <td>12.1983</td>\n",
       "      <td>13.6468</td>\n",
       "      <td>13.8372</td>\n",
       "      <td>1.3675</td>\n",
       "      <td>2.9423</td>\n",
       "      <td>-4.5213</td>\n",
       "      <td>21.4669</td>\n",
       "      <td>9.3225</td>\n",
       "      <td>16.4597</td>\n",
       "      <td>7.9984</td>\n",
       "      <td>-1.7069</td>\n",
       "      <td>-21.4494</td>\n",
       "      <td>6.7806</td>\n",
       "      <td>11.0924</td>\n",
       "      <td>9.9913</td>\n",
       "      <td>14.8421</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>8.9642</td>\n",
       "      <td>16.2572</td>\n",
       "      <td>...</td>\n",
       "      <td>1.218407</td>\n",
       "      <td>1.196663</td>\n",
       "      <td>0.812609</td>\n",
       "      <td>0.889169</td>\n",
       "      <td>0.934415</td>\n",
       "      <td>0.891793</td>\n",
       "      <td>0.975183</td>\n",
       "      <td>1.001917</td>\n",
       "      <td>1.072375</td>\n",
       "      <td>0.903033</td>\n",
       "      <td>0.871822</td>\n",
       "      <td>1.062310</td>\n",
       "      <td>1.090530</td>\n",
       "      <td>0.946887</td>\n",
       "      <td>1.147017</td>\n",
       "      <td>1.034529</td>\n",
       "      <td>0.994725</td>\n",
       "      <td>1.071756</td>\n",
       "      <td>1.037548</td>\n",
       "      <td>1.046615</td>\n",
       "      <td>1.015624</td>\n",
       "      <td>0.996981</td>\n",
       "      <td>0.973589</td>\n",
       "      <td>0.792603</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>0.985295</td>\n",
       "      <td>0.933610</td>\n",
       "      <td>0.839139</td>\n",
       "      <td>1.040965</td>\n",
       "      <td>1.152663</td>\n",
       "      <td>0.844550</td>\n",
       "      <td>0.971426</td>\n",
       "      <td>1.015183</td>\n",
       "      <td>1.076094</td>\n",
       "      <td>0.982869</td>\n",
       "      <td>1.038455</td>\n",
       "      <td>1.023331</td>\n",
       "      <td>0.989312</td>\n",
       "      <td>1.027379</td>\n",
       "      <td>0.865785</td>\n",
       "      <td>0.899928</td>\n",
       "      <td>1.006357</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>1.017684</td>\n",
       "      <td>0.765609</td>\n",
       "      <td>1.161926</td>\n",
       "      <td>1.004696</td>\n",
       "      <td>0.905047</td>\n",
       "      <td>1.036067</td>\n",
       "      <td>0.908603</td>\n",
       "      <td>1.137611</td>\n",
       "      <td>0.870138</td>\n",
       "      <td>1.061127</td>\n",
       "      <td>1.009080</td>\n",
       "      <td>0.917883</td>\n",
       "      <td>0.941532</td>\n",
       "      <td>1.041782</td>\n",
       "      <td>1.231490</td>\n",
       "      <td>0.696698</td>\n",
       "      <td>1.934524</td>\n",
       "      <td>0.967430</td>\n",
       "      <td>1.018207</td>\n",
       "      <td>1.088268</td>\n",
       "      <td>0.877328</td>\n",
       "      <td>1.120142</td>\n",
       "      <td>1.030191</td>\n",
       "      <td>0.912910</td>\n",
       "      <td>1.011907</td>\n",
       "      <td>1.018735</td>\n",
       "      <td>1.063925</td>\n",
       "      <td>1.015428</td>\n",
       "      <td>1.101427</td>\n",
       "      <td>1.083466</td>\n",
       "      <td>0.998691</td>\n",
       "      <td>1.055831</td>\n",
       "      <td>1.173750</td>\n",
       "      <td>1.023453</td>\n",
       "      <td>0.941332</td>\n",
       "      <td>1.194063</td>\n",
       "      <td>0.995053</td>\n",
       "      <td>0.868915</td>\n",
       "      <td>0.927647</td>\n",
       "      <td>0.964357</td>\n",
       "      <td>1.067237</td>\n",
       "      <td>0.999461</td>\n",
       "      <td>1.016186</td>\n",
       "      <td>1.035990</td>\n",
       "      <td>0.899440</td>\n",
       "      <td>1.026965</td>\n",
       "      <td>0.890931</td>\n",
       "      <td>1.006270</td>\n",
       "      <td>0.889903</td>\n",
       "      <td>0.919103</td>\n",
       "      <td>0.987538</td>\n",
       "      <td>0.960883</td>\n",
       "      <td>0.965914</td>\n",
       "      <td>0.942483</td>\n",
       "      <td>0.644645</td>\n",
       "      <td>0.928162</td>\n",
       "      <td>1.009814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>-0.4032</td>\n",
       "      <td>8.0585</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4135</td>\n",
       "      <td>5.4345</td>\n",
       "      <td>13.7003</td>\n",
       "      <td>13.8275</td>\n",
       "      <td>-15.5849</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>28.5708</td>\n",
       "      <td>3.4287</td>\n",
       "      <td>2.7407</td>\n",
       "      <td>8.5524</td>\n",
       "      <td>3.3716</td>\n",
       "      <td>6.9779</td>\n",
       "      <td>13.8910</td>\n",
       "      <td>-11.7684</td>\n",
       "      <td>-2.5586</td>\n",
       "      <td>5.0464</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>-9.2987</td>\n",
       "      <td>7.8755</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>19.3710</td>\n",
       "      <td>11.3702</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>2.7995</td>\n",
       "      <td>5.8434</td>\n",
       "      <td>10.8160</td>\n",
       "      <td>3.6783</td>\n",
       "      <td>-11.1147</td>\n",
       "      <td>1.8730</td>\n",
       "      <td>9.8775</td>\n",
       "      <td>11.7842</td>\n",
       "      <td>1.2444</td>\n",
       "      <td>-47.3797</td>\n",
       "      <td>7.3718</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>34.4014</td>\n",
       "      <td>25.7037</td>\n",
       "      <td>11.8343</td>\n",
       "      <td>13.2256</td>\n",
       "      <td>-4.1083</td>\n",
       "      <td>6.6885</td>\n",
       "      <td>-8.0946</td>\n",
       "      <td>18.5995</td>\n",
       "      <td>19.3219</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>1.9210</td>\n",
       "      <td>8.8682</td>\n",
       "      <td>8.0109</td>\n",
       "      <td>-7.2417</td>\n",
       "      <td>1.7944</td>\n",
       "      <td>-1.3147</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>1.5365</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>7.9344</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>2.2302</td>\n",
       "      <td>40.5632</td>\n",
       "      <td>0.5134</td>\n",
       "      <td>3.1701</td>\n",
       "      <td>20.1068</td>\n",
       "      <td>7.7841</td>\n",
       "      <td>7.0529</td>\n",
       "      <td>3.2709</td>\n",
       "      <td>23.4822</td>\n",
       "      <td>5.5075</td>\n",
       "      <td>13.7814</td>\n",
       "      <td>2.5462</td>\n",
       "      <td>18.1782</td>\n",
       "      <td>0.3683</td>\n",
       "      <td>-4.8210</td>\n",
       "      <td>-5.4850</td>\n",
       "      <td>13.7867</td>\n",
       "      <td>-13.5901</td>\n",
       "      <td>11.0993</td>\n",
       "      <td>7.9022</td>\n",
       "      <td>12.2301</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>6.8852</td>\n",
       "      <td>8.0905</td>\n",
       "      <td>10.9631</td>\n",
       "      <td>11.7569</td>\n",
       "      <td>-1.2722</td>\n",
       "      <td>24.7876</td>\n",
       "      <td>26.6881</td>\n",
       "      <td>...</td>\n",
       "      <td>1.217925</td>\n",
       "      <td>1.114817</td>\n",
       "      <td>0.985909</td>\n",
       "      <td>0.990954</td>\n",
       "      <td>1.110601</td>\n",
       "      <td>1.112495</td>\n",
       "      <td>1.052948</td>\n",
       "      <td>0.882170</td>\n",
       "      <td>0.977026</td>\n",
       "      <td>0.897559</td>\n",
       "      <td>1.030325</td>\n",
       "      <td>0.984106</td>\n",
       "      <td>0.886807</td>\n",
       "      <td>0.983530</td>\n",
       "      <td>0.973513</td>\n",
       "      <td>1.012811</td>\n",
       "      <td>1.163963</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>2.030026</td>\n",
       "      <td>0.870445</td>\n",
       "      <td>1.953606</td>\n",
       "      <td>1.277193</td>\n",
       "      <td>1.043907</td>\n",
       "      <td>1.195843</td>\n",
       "      <td>1.021242</td>\n",
       "      <td>0.952271</td>\n",
       "      <td>0.999064</td>\n",
       "      <td>1.210541</td>\n",
       "      <td>0.978541</td>\n",
       "      <td>0.960561</td>\n",
       "      <td>1.225596</td>\n",
       "      <td>0.979548</td>\n",
       "      <td>0.871125</td>\n",
       "      <td>1.032325</td>\n",
       "      <td>0.978874</td>\n",
       "      <td>1.014035</td>\n",
       "      <td>1.022682</td>\n",
       "      <td>1.000920</td>\n",
       "      <td>0.999924</td>\n",
       "      <td>0.997974</td>\n",
       "      <td>1.028514</td>\n",
       "      <td>1.052611</td>\n",
       "      <td>0.927140</td>\n",
       "      <td>0.976393</td>\n",
       "      <td>1.066150</td>\n",
       "      <td>0.926612</td>\n",
       "      <td>0.852355</td>\n",
       "      <td>0.988479</td>\n",
       "      <td>0.871881</td>\n",
       "      <td>0.931475</td>\n",
       "      <td>0.898107</td>\n",
       "      <td>1.052355</td>\n",
       "      <td>1.037122</td>\n",
       "      <td>0.991057</td>\n",
       "      <td>0.913065</td>\n",
       "      <td>1.026026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.229756</td>\n",
       "      <td>2.602401</td>\n",
       "      <td>0.923342</td>\n",
       "      <td>1.083392</td>\n",
       "      <td>0.962923</td>\n",
       "      <td>0.891192</td>\n",
       "      <td>1.073670</td>\n",
       "      <td>0.999163</td>\n",
       "      <td>0.533389</td>\n",
       "      <td>1.105764</td>\n",
       "      <td>0.971632</td>\n",
       "      <td>1.061841</td>\n",
       "      <td>1.058222</td>\n",
       "      <td>1.683168</td>\n",
       "      <td>1.187939</td>\n",
       "      <td>1.005410</td>\n",
       "      <td>1.009463</td>\n",
       "      <td>0.976819</td>\n",
       "      <td>0.409186</td>\n",
       "      <td>0.941226</td>\n",
       "      <td>0.956960</td>\n",
       "      <td>0.939387</td>\n",
       "      <td>1.003959</td>\n",
       "      <td>1.331807</td>\n",
       "      <td>0.981848</td>\n",
       "      <td>1.048711</td>\n",
       "      <td>0.995883</td>\n",
       "      <td>1.111374</td>\n",
       "      <td>0.558398</td>\n",
       "      <td>1.069292</td>\n",
       "      <td>0.952728</td>\n",
       "      <td>0.960437</td>\n",
       "      <td>0.866660</td>\n",
       "      <td>0.886594</td>\n",
       "      <td>0.988293</td>\n",
       "      <td>0.992117</td>\n",
       "      <td>1.019159</td>\n",
       "      <td>1.135200</td>\n",
       "      <td>0.995041</td>\n",
       "      <td>1.048840</td>\n",
       "      <td>0.955403</td>\n",
       "      <td>0.872108</td>\n",
       "      <td>1.009814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>-0.3249</td>\n",
       "      <td>-11.2648</td>\n",
       "      <td>14.1929</td>\n",
       "      <td>7.3124</td>\n",
       "      <td>7.5244</td>\n",
       "      <td>14.6472</td>\n",
       "      <td>7.6782</td>\n",
       "      <td>-1.7395</td>\n",
       "      <td>4.7011</td>\n",
       "      <td>20.4775</td>\n",
       "      <td>17.7559</td>\n",
       "      <td>18.1377</td>\n",
       "      <td>1.2145</td>\n",
       "      <td>3.5137</td>\n",
       "      <td>5.6777</td>\n",
       "      <td>13.2177</td>\n",
       "      <td>-7.9940</td>\n",
       "      <td>-2.9029</td>\n",
       "      <td>5.8463</td>\n",
       "      <td>6.1439</td>\n",
       "      <td>-11.1025</td>\n",
       "      <td>12.4858</td>\n",
       "      <td>-2.2871</td>\n",
       "      <td>19.0422</td>\n",
       "      <td>11.0449</td>\n",
       "      <td>4.1087</td>\n",
       "      <td>4.6974</td>\n",
       "      <td>6.9346</td>\n",
       "      <td>10.8917</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>-13.5174</td>\n",
       "      <td>2.2439</td>\n",
       "      <td>11.5283</td>\n",
       "      <td>12.0406</td>\n",
       "      <td>4.1006</td>\n",
       "      <td>-7.9078</td>\n",
       "      <td>11.1405</td>\n",
       "      <td>-5.7864</td>\n",
       "      <td>20.7477</td>\n",
       "      <td>6.8874</td>\n",
       "      <td>12.9143</td>\n",
       "      <td>19.5856</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>6.4059</td>\n",
       "      <td>9.3124</td>\n",
       "      <td>6.2846</td>\n",
       "      <td>15.6372</td>\n",
       "      <td>5.8200</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>9.1854</td>\n",
       "      <td>12.5963</td>\n",
       "      <td>-10.3734</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>5.8042</td>\n",
       "      <td>3.7163</td>\n",
       "      <td>-1.1016</td>\n",
       "      <td>7.3667</td>\n",
       "      <td>9.8565</td>\n",
       "      <td>5.0228</td>\n",
       "      <td>-5.7828</td>\n",
       "      <td>2.3612</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>6.3577</td>\n",
       "      <td>12.1719</td>\n",
       "      <td>19.7312</td>\n",
       "      <td>19.4465</td>\n",
       "      <td>4.5048</td>\n",
       "      <td>23.2378</td>\n",
       "      <td>6.3191</td>\n",
       "      <td>12.8046</td>\n",
       "      <td>7.4729</td>\n",
       "      <td>15.7811</td>\n",
       "      <td>13.3529</td>\n",
       "      <td>10.1852</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>19.0773</td>\n",
       "      <td>-4.4577</td>\n",
       "      <td>9.5413</td>\n",
       "      <td>11.9052</td>\n",
       "      <td>2.1447</td>\n",
       "      <td>-22.4038</td>\n",
       "      <td>7.0883</td>\n",
       "      <td>14.1613</td>\n",
       "      <td>10.5080</td>\n",
       "      <td>14.2621</td>\n",
       "      <td>0.2647</td>\n",
       "      <td>20.4031</td>\n",
       "      <td>17.0360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955635</td>\n",
       "      <td>1.128926</td>\n",
       "      <td>1.006334</td>\n",
       "      <td>0.888970</td>\n",
       "      <td>0.929048</td>\n",
       "      <td>0.954802</td>\n",
       "      <td>0.870958</td>\n",
       "      <td>0.894696</td>\n",
       "      <td>0.966570</td>\n",
       "      <td>1.071074</td>\n",
       "      <td>0.900090</td>\n",
       "      <td>1.070114</td>\n",
       "      <td>0.919485</td>\n",
       "      <td>1.056092</td>\n",
       "      <td>1.115416</td>\n",
       "      <td>1.036028</td>\n",
       "      <td>0.859134</td>\n",
       "      <td>0.922808</td>\n",
       "      <td>1.020694</td>\n",
       "      <td>1.017321</td>\n",
       "      <td>1.190053</td>\n",
       "      <td>0.990609</td>\n",
       "      <td>1.022414</td>\n",
       "      <td>0.793201</td>\n",
       "      <td>0.999875</td>\n",
       "      <td>1.060243</td>\n",
       "      <td>1.004738</td>\n",
       "      <td>1.228803</td>\n",
       "      <td>1.025767</td>\n",
       "      <td>1.052819</td>\n",
       "      <td>1.093299</td>\n",
       "      <td>1.335733</td>\n",
       "      <td>1.029581</td>\n",
       "      <td>1.004433</td>\n",
       "      <td>1.015466</td>\n",
       "      <td>1.121353</td>\n",
       "      <td>1.343313</td>\n",
       "      <td>0.991786</td>\n",
       "      <td>0.983804</td>\n",
       "      <td>1.007997</td>\n",
       "      <td>0.932281</td>\n",
       "      <td>0.774171</td>\n",
       "      <td>1.057728</td>\n",
       "      <td>1.072669</td>\n",
       "      <td>1.015130</td>\n",
       "      <td>1.011970</td>\n",
       "      <td>0.949724</td>\n",
       "      <td>0.951553</td>\n",
       "      <td>1.013237</td>\n",
       "      <td>1.018472</td>\n",
       "      <td>0.994290</td>\n",
       "      <td>1.018201</td>\n",
       "      <td>0.962919</td>\n",
       "      <td>0.984385</td>\n",
       "      <td>1.073277</td>\n",
       "      <td>0.999343</td>\n",
       "      <td>0.991762</td>\n",
       "      <td>0.888863</td>\n",
       "      <td>0.894455</td>\n",
       "      <td>1.436729</td>\n",
       "      <td>0.965233</td>\n",
       "      <td>1.041557</td>\n",
       "      <td>0.986033</td>\n",
       "      <td>0.919970</td>\n",
       "      <td>1.101490</td>\n",
       "      <td>1.054443</td>\n",
       "      <td>1.082102</td>\n",
       "      <td>1.011907</td>\n",
       "      <td>1.022511</td>\n",
       "      <td>1.056645</td>\n",
       "      <td>0.790017</td>\n",
       "      <td>1.181691</td>\n",
       "      <td>0.792797</td>\n",
       "      <td>1.004569</td>\n",
       "      <td>1.057686</td>\n",
       "      <td>1.018836</td>\n",
       "      <td>0.850479</td>\n",
       "      <td>0.945035</td>\n",
       "      <td>1.135159</td>\n",
       "      <td>0.955382</td>\n",
       "      <td>0.821828</td>\n",
       "      <td>0.902246</td>\n",
       "      <td>0.965016</td>\n",
       "      <td>1.004134</td>\n",
       "      <td>1.055433</td>\n",
       "      <td>0.979087</td>\n",
       "      <td>1.010823</td>\n",
       "      <td>0.961735</td>\n",
       "      <td>1.004140</td>\n",
       "      <td>0.842703</td>\n",
       "      <td>1.006270</td>\n",
       "      <td>1.018511</td>\n",
       "      <td>0.972071</td>\n",
       "      <td>0.987538</td>\n",
       "      <td>0.960883</td>\n",
       "      <td>1.028244</td>\n",
       "      <td>0.975031</td>\n",
       "      <td>0.959222</td>\n",
       "      <td>1.105710</td>\n",
       "      <td>0.981893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>2.3061</td>\n",
       "      <td>2.8102</td>\n",
       "      <td>13.8463</td>\n",
       "      <td>11.9704</td>\n",
       "      <td>6.4569</td>\n",
       "      <td>14.8372</td>\n",
       "      <td>10.7430</td>\n",
       "      <td>-0.4299</td>\n",
       "      <td>15.9426</td>\n",
       "      <td>13.7257</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>12.5579</td>\n",
       "      <td>6.8202</td>\n",
       "      <td>2.7229</td>\n",
       "      <td>12.1354</td>\n",
       "      <td>13.7367</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>-0.9059</td>\n",
       "      <td>5.9070</td>\n",
       "      <td>2.8407</td>\n",
       "      <td>-15.2398</td>\n",
       "      <td>10.4407</td>\n",
       "      <td>-2.5731</td>\n",
       "      <td>6.1796</td>\n",
       "      <td>10.6093</td>\n",
       "      <td>-5.9158</td>\n",
       "      <td>8.1723</td>\n",
       "      <td>2.8521</td>\n",
       "      <td>9.1738</td>\n",
       "      <td>0.6665</td>\n",
       "      <td>-3.8294</td>\n",
       "      <td>-1.0370</td>\n",
       "      <td>11.7770</td>\n",
       "      <td>11.2834</td>\n",
       "      <td>8.0485</td>\n",
       "      <td>-24.6840</td>\n",
       "      <td>12.7404</td>\n",
       "      <td>-35.1659</td>\n",
       "      <td>0.7613</td>\n",
       "      <td>8.3838</td>\n",
       "      <td>12.6832</td>\n",
       "      <td>9.5503</td>\n",
       "      <td>1.7895</td>\n",
       "      <td>5.2091</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>12.3972</td>\n",
       "      <td>14.4698</td>\n",
       "      <td>6.5850</td>\n",
       "      <td>3.3164</td>\n",
       "      <td>9.4638</td>\n",
       "      <td>15.7820</td>\n",
       "      <td>-25.0222</td>\n",
       "      <td>3.4418</td>\n",
       "      <td>-4.3923</td>\n",
       "      <td>8.6464</td>\n",
       "      <td>6.3072</td>\n",
       "      <td>5.6221</td>\n",
       "      <td>23.6143</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>-3.9989</td>\n",
       "      <td>4.0462</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.2516</td>\n",
       "      <td>24.4187</td>\n",
       "      <td>4.5290</td>\n",
       "      <td>15.4235</td>\n",
       "      <td>11.6875</td>\n",
       "      <td>23.6273</td>\n",
       "      <td>4.0806</td>\n",
       "      <td>15.2733</td>\n",
       "      <td>0.7839</td>\n",
       "      <td>10.5404</td>\n",
       "      <td>1.6212</td>\n",
       "      <td>-5.2896</td>\n",
       "      <td>1.6027</td>\n",
       "      <td>17.9762</td>\n",
       "      <td>-2.3174</td>\n",
       "      <td>15.6298</td>\n",
       "      <td>4.5474</td>\n",
       "      <td>7.5509</td>\n",
       "      <td>-7.5866</td>\n",
       "      <td>7.0364</td>\n",
       "      <td>14.4027</td>\n",
       "      <td>10.7795</td>\n",
       "      <td>7.2887</td>\n",
       "      <td>-1.0930</td>\n",
       "      <td>11.3596</td>\n",
       "      <td>18.1486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836494</td>\n",
       "      <td>1.068056</td>\n",
       "      <td>0.973402</td>\n",
       "      <td>0.901728</td>\n",
       "      <td>1.048108</td>\n",
       "      <td>0.876442</td>\n",
       "      <td>1.042827</td>\n",
       "      <td>1.140052</td>\n",
       "      <td>0.982386</td>\n",
       "      <td>1.026151</td>\n",
       "      <td>0.905416</td>\n",
       "      <td>0.947613</td>\n",
       "      <td>0.455978</td>\n",
       "      <td>0.886809</td>\n",
       "      <td>1.016690</td>\n",
       "      <td>1.058375</td>\n",
       "      <td>1.174237</td>\n",
       "      <td>1.131731</td>\n",
       "      <td>1.076462</td>\n",
       "      <td>1.011180</td>\n",
       "      <td>0.977994</td>\n",
       "      <td>0.915706</td>\n",
       "      <td>0.973242</td>\n",
       "      <td>1.010823</td>\n",
       "      <td>1.056169</td>\n",
       "      <td>0.940405</td>\n",
       "      <td>1.071112</td>\n",
       "      <td>0.852675</td>\n",
       "      <td>1.049827</td>\n",
       "      <td>1.021444</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.978014</td>\n",
       "      <td>1.066932</td>\n",
       "      <td>0.926863</td>\n",
       "      <td>1.012418</td>\n",
       "      <td>0.954173</td>\n",
       "      <td>1.023331</td>\n",
       "      <td>1.007037</td>\n",
       "      <td>1.033908</td>\n",
       "      <td>0.997974</td>\n",
       "      <td>0.899401</td>\n",
       "      <td>0.790479</td>\n",
       "      <td>0.787578</td>\n",
       "      <td>0.994356</td>\n",
       "      <td>0.861709</td>\n",
       "      <td>0.889981</td>\n",
       "      <td>1.069513</td>\n",
       "      <td>1.011655</td>\n",
       "      <td>0.821320</td>\n",
       "      <td>1.073566</td>\n",
       "      <td>0.834580</td>\n",
       "      <td>2.291878</td>\n",
       "      <td>1.072888</td>\n",
       "      <td>0.974842</td>\n",
       "      <td>1.103723</td>\n",
       "      <td>1.019714</td>\n",
       "      <td>0.944826</td>\n",
       "      <td>1.015126</td>\n",
       "      <td>1.131716</td>\n",
       "      <td>1.033670</td>\n",
       "      <td>1.021356</td>\n",
       "      <td>1.006981</td>\n",
       "      <td>0.964882</td>\n",
       "      <td>1.049607</td>\n",
       "      <td>0.892744</td>\n",
       "      <td>0.783332</td>\n",
       "      <td>0.939763</td>\n",
       "      <td>0.979965</td>\n",
       "      <td>0.974896</td>\n",
       "      <td>0.939916</td>\n",
       "      <td>0.737724</td>\n",
       "      <td>0.841794</td>\n",
       "      <td>1.015097</td>\n",
       "      <td>0.995473</td>\n",
       "      <td>0.988140</td>\n",
       "      <td>1.014332</td>\n",
       "      <td>0.863707</td>\n",
       "      <td>0.918191</td>\n",
       "      <td>0.919336</td>\n",
       "      <td>0.992274</td>\n",
       "      <td>0.848997</td>\n",
       "      <td>0.891852</td>\n",
       "      <td>1.036800</td>\n",
       "      <td>0.895976</td>\n",
       "      <td>0.883443</td>\n",
       "      <td>0.986104</td>\n",
       "      <td>0.952094</td>\n",
       "      <td>1.026108</td>\n",
       "      <td>0.995137</td>\n",
       "      <td>0.749434</td>\n",
       "      <td>0.947081</td>\n",
       "      <td>1.020585</td>\n",
       "      <td>1.006031</td>\n",
       "      <td>1.019159</td>\n",
       "      <td>1.340088</td>\n",
       "      <td>0.956568</td>\n",
       "      <td>1.032904</td>\n",
       "      <td>1.054807</td>\n",
       "      <td>1.106460</td>\n",
       "      <td>1.023166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>-9.4458</td>\n",
       "      <td>-12.1419</td>\n",
       "      <td>13.8481</td>\n",
       "      <td>7.8895</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>15.0553</td>\n",
       "      <td>8.4871</td>\n",
       "      <td>-3.0680</td>\n",
       "      <td>6.5263</td>\n",
       "      <td>11.3152</td>\n",
       "      <td>21.4246</td>\n",
       "      <td>18.9608</td>\n",
       "      <td>10.1102</td>\n",
       "      <td>2.7142</td>\n",
       "      <td>14.2080</td>\n",
       "      <td>13.5433</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>-3.3423</td>\n",
       "      <td>5.9015</td>\n",
       "      <td>7.9352</td>\n",
       "      <td>-3.1582</td>\n",
       "      <td>9.4668</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>19.3239</td>\n",
       "      <td>12.4057</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>2.7922</td>\n",
       "      <td>5.8184</td>\n",
       "      <td>19.3038</td>\n",
       "      <td>1.4450</td>\n",
       "      <td>-5.5963</td>\n",
       "      <td>14.0685</td>\n",
       "      <td>11.9171</td>\n",
       "      <td>11.5111</td>\n",
       "      <td>6.9087</td>\n",
       "      <td>-65.4863</td>\n",
       "      <td>13.8657</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>-0.1346</td>\n",
       "      <td>14.4268</td>\n",
       "      <td>13.3273</td>\n",
       "      <td>10.4857</td>\n",
       "      <td>-1.4367</td>\n",
       "      <td>5.7555</td>\n",
       "      <td>-8.5414</td>\n",
       "      <td>14.1482</td>\n",
       "      <td>16.9840</td>\n",
       "      <td>6.1812</td>\n",
       "      <td>1.9548</td>\n",
       "      <td>9.2048</td>\n",
       "      <td>8.6591</td>\n",
       "      <td>-27.7439</td>\n",
       "      <td>-0.4952</td>\n",
       "      <td>-1.7839</td>\n",
       "      <td>5.2670</td>\n",
       "      <td>-4.3205</td>\n",
       "      <td>6.9860</td>\n",
       "      <td>1.6184</td>\n",
       "      <td>5.0301</td>\n",
       "      <td>-3.2431</td>\n",
       "      <td>40.1236</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>-0.7264</td>\n",
       "      <td>4.5886</td>\n",
       "      <td>-4.5346</td>\n",
       "      <td>23.3521</td>\n",
       "      <td>1.0273</td>\n",
       "      <td>19.1600</td>\n",
       "      <td>7.1734</td>\n",
       "      <td>14.3937</td>\n",
       "      <td>2.9598</td>\n",
       "      <td>13.3317</td>\n",
       "      <td>-9.2587</td>\n",
       "      <td>-6.7075</td>\n",
       "      <td>7.8984</td>\n",
       "      <td>14.5265</td>\n",
       "      <td>7.0799</td>\n",
       "      <td>20.1670</td>\n",
       "      <td>8.0053</td>\n",
       "      <td>3.7954</td>\n",
       "      <td>-39.7997</td>\n",
       "      <td>7.0065</td>\n",
       "      <td>9.3627</td>\n",
       "      <td>10.4316</td>\n",
       "      <td>14.0553</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>14.7246</td>\n",
       "      <td>35.2988</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006702</td>\n",
       "      <td>0.838253</td>\n",
       "      <td>0.973402</td>\n",
       "      <td>1.127910</td>\n",
       "      <td>1.863278</td>\n",
       "      <td>0.891793</td>\n",
       "      <td>0.933553</td>\n",
       "      <td>1.124240</td>\n",
       "      <td>0.982386</td>\n",
       "      <td>0.969969</td>\n",
       "      <td>0.994928</td>\n",
       "      <td>1.055283</td>\n",
       "      <td>0.886807</td>\n",
       "      <td>0.983530</td>\n",
       "      <td>0.973513</td>\n",
       "      <td>0.987938</td>\n",
       "      <td>1.143667</td>\n",
       "      <td>1.026452</td>\n",
       "      <td>1.096372</td>\n",
       "      <td>1.142997</td>\n",
       "      <td>0.978979</td>\n",
       "      <td>0.905360</td>\n",
       "      <td>0.949655</td>\n",
       "      <td>1.216595</td>\n",
       "      <td>0.935716</td>\n",
       "      <td>0.937493</td>\n",
       "      <td>0.943381</td>\n",
       "      <td>0.850976</td>\n",
       "      <td>0.967923</td>\n",
       "      <td>1.044150</td>\n",
       "      <td>1.065582</td>\n",
       "      <td>1.029415</td>\n",
       "      <td>0.945220</td>\n",
       "      <td>0.931063</td>\n",
       "      <td>1.021273</td>\n",
       "      <td>0.993897</td>\n",
       "      <td>0.989945</td>\n",
       "      <td>1.025924</td>\n",
       "      <td>0.837469</td>\n",
       "      <td>1.143594</td>\n",
       "      <td>0.985984</td>\n",
       "      <td>1.052611</td>\n",
       "      <td>1.016399</td>\n",
       "      <td>1.033911</td>\n",
       "      <td>1.699006</td>\n",
       "      <td>0.767872</td>\n",
       "      <td>1.004326</td>\n",
       "      <td>0.979263</td>\n",
       "      <td>1.171007</td>\n",
       "      <td>1.043875</td>\n",
       "      <td>0.947274</td>\n",
       "      <td>0.982413</td>\n",
       "      <td>0.855327</td>\n",
       "      <td>0.961186</td>\n",
       "      <td>1.098946</td>\n",
       "      <td>1.065269</td>\n",
       "      <td>0.983013</td>\n",
       "      <td>1.231490</td>\n",
       "      <td>0.999610</td>\n",
       "      <td>1.013785</td>\n",
       "      <td>0.674985</td>\n",
       "      <td>1.010778</td>\n",
       "      <td>0.993879</td>\n",
       "      <td>0.924252</td>\n",
       "      <td>1.094295</td>\n",
       "      <td>0.997093</td>\n",
       "      <td>0.992454</td>\n",
       "      <td>0.995211</td>\n",
       "      <td>1.061841</td>\n",
       "      <td>0.537980</td>\n",
       "      <td>0.917596</td>\n",
       "      <td>0.895950</td>\n",
       "      <td>0.852852</td>\n",
       "      <td>0.997740</td>\n",
       "      <td>1.058544</td>\n",
       "      <td>1.029096</td>\n",
       "      <td>1.035228</td>\n",
       "      <td>1.073733</td>\n",
       "      <td>0.900018</td>\n",
       "      <td>1.016959</td>\n",
       "      <td>0.891546</td>\n",
       "      <td>1.016351</td>\n",
       "      <td>0.948608</td>\n",
       "      <td>0.856314</td>\n",
       "      <td>0.883443</td>\n",
       "      <td>0.992778</td>\n",
       "      <td>0.997854</td>\n",
       "      <td>1.026108</td>\n",
       "      <td>1.013438</td>\n",
       "      <td>0.866660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.994185</td>\n",
       "      <td>0.984521</td>\n",
       "      <td>1.006857</td>\n",
       "      <td>1.040710</td>\n",
       "      <td>1.077709</td>\n",
       "      <td>1.089686</td>\n",
       "      <td>1.036233</td>\n",
       "      <td>0.857310</td>\n",
       "      <td>1.009814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7   var_8   var_9  var_10   var_11   var_12   var_13  var_14  \\\n",
       "0  18.6266 -4.9200  5.7470  2.9252   3.1821  14.0137   0.5745  8.7989   \n",
       "1  16.5338  3.1468  8.0851 -0.4032   8.0585  14.0239   8.4135  5.4345   \n",
       "2  14.6155 -4.9193  5.9525 -0.3249 -11.2648  14.1929   7.3124  7.5244   \n",
       "3  14.9250 -5.8609  8.2450  2.3061   2.8102  13.8463  11.9704  6.4569   \n",
       "4  19.2514  6.2654  7.6784 -9.4458 -12.1419  13.8481   7.8895  7.7894   \n",
       "\n",
       "    var_15   var_16   var_17   var_18   var_19   var_20   var_21   var_22  \\\n",
       "0  14.5691   5.7487  -7.2393   4.2840  30.7133  10.5350  16.2191   2.5791   \n",
       "1  13.7003  13.8275 -15.5849   7.8000  28.5708   3.4287   2.7407   8.5524   \n",
       "2  14.6472   7.6782  -1.7395   4.7011  20.4775  17.7559  18.1377   1.2145   \n",
       "3  14.8372  10.7430  -0.4299  15.9426  13.7257  20.3010  12.5579   6.8202   \n",
       "4  15.0553   8.4871  -3.0680   6.5263  11.3152  21.4246  18.9608  10.1102   \n",
       "\n",
       "   var_23   var_24   var_25   var_26  var_27  var_28  var_29   var_30  \\\n",
       "0  2.4716  14.3831  13.4325  -5.1488 -0.4073  4.9306  5.9965  -0.3085   \n",
       "1  3.3716   6.9779  13.8910 -11.7684 -2.5586  5.0464  0.5481  -9.2987   \n",
       "2  3.5137   5.6777  13.2177  -7.9940 -2.9029  5.8463  6.1439 -11.1025   \n",
       "3  2.7229  12.1354  13.7367   0.8135 -0.9059  5.9070  2.8407 -15.2398   \n",
       "4  2.7142  14.2080  13.5433   3.1736 -3.3423  5.9015  7.9352  -3.1582   \n",
       "\n",
       "    var_31  var_32   var_33   var_34   var_35  var_36  var_37   var_38  \\\n",
       "0  12.9041 -3.8766  16.8911  11.1920  10.5785  0.6764  7.8871   4.6667   \n",
       "1   7.8755  1.2859  19.3710  11.3702   0.7399  2.7995  5.8434  10.8160   \n",
       "2  12.4858 -2.2871  19.0422  11.0449   4.1087  4.6974  6.9346  10.8917   \n",
       "3  10.4407 -2.5731   6.1796  10.6093  -5.9158  8.1723  2.8521   9.1738   \n",
       "4   9.4668 -0.0083  19.3239  12.4057   0.6329  2.7922  5.8184  19.3038   \n",
       "\n",
       "   var_39   var_40   var_41   var_42   var_43   var_44   var_45   var_46  \\\n",
       "0  3.8743  -5.2387   7.3746  11.5767  12.0446  11.6418  -7.0170   5.9226   \n",
       "1  3.6783 -11.1147   1.8730   9.8775  11.7842   1.2444 -47.3797   7.3718   \n",
       "2  0.9003 -13.5174   2.2439  11.5283  12.0406   4.1006  -7.9078  11.1405   \n",
       "3  0.6665  -3.8294  -1.0370  11.7770  11.2834   8.0485 -24.6840  12.7404   \n",
       "4  1.4450  -5.5963  14.0685  11.9171  11.5111   6.9087 -65.4863  13.8657   \n",
       "\n",
       "    var_47   var_48   var_49   var_50   var_51  var_52  var_53  var_54  \\\n",
       "0 -14.2136  16.0283   5.3253  12.9194  29.0460 -0.6940  5.1736 -0.7474   \n",
       "1   0.1948  34.4014  25.7037  11.8343  13.2256 -4.1083  6.6885 -8.0946   \n",
       "2  -5.7864  20.7477   6.8874  12.9143  19.5856  0.7268  6.4059  9.3124   \n",
       "3 -35.1659   0.7613   8.3838  12.6832   9.5503  1.7895  5.2091  8.0913   \n",
       "4   0.0444  -0.1346  14.4268  13.3273  10.4857 -1.4367  5.7555 -8.5414   \n",
       "\n",
       "    var_55   var_56  var_57  var_58   var_59   var_60   var_61  var_62  \\\n",
       "0  14.8322  11.2668  5.3822  2.0183  10.1166  16.1828   4.9590  2.0771   \n",
       "1  18.5995  19.3219  7.0118  1.9210   8.8682   8.0109  -7.2417  1.7944   \n",
       "2   6.2846  15.6372  5.8200  1.1000   9.1854  12.5963 -10.3734  0.8748   \n",
       "3  12.3972  14.4698  6.5850  3.3164   9.4638  15.7820 -25.0222  3.4418   \n",
       "4  14.1482  16.9840  6.1812  1.9548   9.2048   8.6591 -27.7439 -0.4952   \n",
       "\n",
       "   var_63  var_64  var_65  var_66   var_67  var_68  var_69   var_70  var_71  \\\n",
       "0 -0.2154  8.6748  9.5319  5.8056  22.4321  5.0109 -4.7010  21.6374  0.5663   \n",
       "1 -1.3147  8.1042  1.5365  5.4007   7.9344  5.0220  2.2302  40.5632  0.5134   \n",
       "2  5.8042  3.7163 -1.1016  7.3667   9.8565  5.0228 -5.7828   2.3612  0.8520   \n",
       "3 -4.3923  8.6464  6.3072  5.6221  23.6143  5.0220 -3.9989   4.0462  0.2500   \n",
       "4 -1.7839  5.2670 -4.3205  6.9860   1.6184  5.0301 -3.2431  40.1236  0.7737   \n",
       "\n",
       "   var_72   var_73   var_74   var_75   var_76   var_77  var_78   var_79  \\\n",
       "0  5.1999   8.8600  43.1127  18.3816  -2.3440  23.4104  6.5199  12.1983   \n",
       "1  3.1701  20.1068   7.7841   7.0529   3.2709  23.4822  5.5075  13.7814   \n",
       "2  6.3577  12.1719  19.7312  19.4465   4.5048  23.2378  6.3191  12.8046   \n",
       "3  1.2516  24.4187   4.5290  15.4235  11.6875  23.6273  4.0806  15.2733   \n",
       "4 -0.7264   4.5886  -4.5346  23.3521   1.0273  19.1600  7.1734  14.3937   \n",
       "\n",
       "    var_80   var_81   var_82   var_83  var_84   var_85   var_86   var_87  \\\n",
       "0  13.6468  13.8372   1.3675   2.9423 -4.5213  21.4669   9.3225  16.4597   \n",
       "1   2.5462  18.1782   0.3683  -4.8210 -5.4850  13.7867 -13.5901  11.0993   \n",
       "2   7.4729  15.7811  13.3529  10.1852  5.4604  19.0773  -4.4577   9.5413   \n",
       "3   0.7839  10.5404   1.6212  -5.2896  1.6027  17.9762  -2.3174  15.6298   \n",
       "4   2.9598  13.3317  -9.2587  -6.7075  7.8984  14.5265   7.0799  20.1670   \n",
       "\n",
       "    var_88   var_89   var_90  var_91   var_92   var_93   var_94  var_95  \\\n",
       "0   7.9984  -1.7069 -21.4494  6.7806  11.0924   9.9913  14.8421  0.1812   \n",
       "1   7.9022  12.2301   0.4768  6.8852   8.0905  10.9631  11.7569 -1.2722   \n",
       "2  11.9052   2.1447 -22.4038  7.0883  14.1613  10.5080  14.2621  0.2647   \n",
       "3   4.5474   7.5509  -7.5866  7.0364  14.4027  10.7795   7.2887 -1.0930   \n",
       "4   8.0053   3.7954 -39.7997  7.0065   9.3627  10.4316  14.0553  0.0213   \n",
       "\n",
       "    var_96   var_97      ...       var_196_ratio  var_197_ratio  \\\n",
       "0   8.9642  16.2572      ...            1.218407       1.196663   \n",
       "1  24.7876  26.6881      ...            1.217925       1.114817   \n",
       "2  20.4031  17.0360      ...            0.955635       1.128926   \n",
       "3  11.3596  18.1486      ...            0.836494       1.068056   \n",
       "4  14.7246  35.2988      ...            1.006702       0.838253   \n",
       "\n",
       "   var_198_ratio  var_2_ratio  var_22_ratio  var_24_ratio  var_25_ratio  \\\n",
       "0       0.812609     0.889169      0.934415      0.891793      0.975183   \n",
       "1       0.985909     0.990954      1.110601      1.112495      1.052948   \n",
       "2       1.006334     0.888970      0.929048      0.954802      0.870958   \n",
       "3       0.973402     0.901728      1.048108      0.876442      1.042827   \n",
       "4       0.973402     1.127910      1.863278      0.891793      0.933553   \n",
       "\n",
       "   var_26_ratio  var_28_ratio  var_3_ratio  var_32_ratio  var_34_ratio  \\\n",
       "0      1.001917      1.072375     0.903033      0.871822      1.062310   \n",
       "1      0.882170      0.977026     0.897559      1.030325      0.984106   \n",
       "2      0.894696      0.966570     1.071074      0.900090      1.070114   \n",
       "3      1.140052      0.982386     1.026151      0.905416      0.947613   \n",
       "4      1.124240      0.982386     0.969969      0.994928      1.055283   \n",
       "\n",
       "   var_35_ratio  var_36_ratio  var_37_ratio  var_39_ratio  var_4_ratio  \\\n",
       "0      1.090530      0.946887      1.147017      1.034529     0.994725   \n",
       "1      0.886807      0.983530      0.973513      1.012811     1.163963   \n",
       "2      0.919485      1.056092      1.115416      1.036028     0.859134   \n",
       "3      0.455978      0.886809      1.016690      1.058375     1.174237   \n",
       "4      0.886807      0.983530      0.973513      0.987938     1.143667   \n",
       "\n",
       "   var_40_ratio  var_42_ratio  var_43_ratio  var_44_ratio  var_48_ratio  \\\n",
       "0      1.071756      1.037548      1.046615      1.015624      0.996981   \n",
       "1      0.894309      2.030026      0.870445      1.953606      1.277193   \n",
       "2      0.922808      1.020694      1.017321      1.190053      0.990609   \n",
       "3      1.131731      1.076462      1.011180      0.977994      0.915706   \n",
       "4      1.026452      1.096372      1.142997      0.978979      0.905360   \n",
       "\n",
       "   var_49_ratio  var_5_ratio  var_50_ratio  var_51_ratio  var_52_ratio  \\\n",
       "0      0.973589     0.792603      0.999875      0.985295      0.933610   \n",
       "1      1.043907     1.195843      1.021242      0.952271      0.999064   \n",
       "2      1.022414     0.793201      0.999875      1.060243      1.004738   \n",
       "3      0.973242     1.010823      1.056169      0.940405      1.071112   \n",
       "4      0.949655     1.216595      0.935716      0.937493      0.943381   \n",
       "\n",
       "   var_53_ratio  var_55_ratio  var_56_ratio  var_59_ratio  var_6_ratio  \\\n",
       "0      0.839139      1.040965      1.152663      0.844550     0.971426   \n",
       "1      1.210541      0.978541      0.960561      1.225596     0.979548   \n",
       "2      1.228803      1.025767      1.052819      1.093299     1.335733   \n",
       "3      0.852675      1.049827      1.021444      0.900933     0.978014   \n",
       "4      0.850976      0.967923      1.044150      1.065582     1.029415   \n",
       "\n",
       "   var_60_ratio  var_61_ratio  var_62_ratio  var_63_ratio  var_64_ratio  \\\n",
       "0      1.015183      1.076094      0.982869      1.038455      1.023331   \n",
       "1      0.871125      1.032325      0.978874      1.014035      1.022682   \n",
       "2      1.029581      1.004433      1.015466      1.121353      1.343313   \n",
       "3      1.066932      0.926863      1.012418      0.954173      1.023331   \n",
       "4      0.945220      0.931063      1.021273      0.993897      0.989945   \n",
       "\n",
       "   var_66_ratio  var_67_ratio  var_68_ratio  var_69_ratio  var_70_ratio  \\\n",
       "0      0.989312      1.027379      0.865785      0.899928      1.006357   \n",
       "1      1.000920      0.999924      0.997974      1.028514      1.052611   \n",
       "2      0.991786      0.983804      1.007997      0.932281      0.774171   \n",
       "3      1.007037      1.033908      0.997974      0.899401      0.790479   \n",
       "4      1.025924      0.837469      1.143594      0.985984      1.052611   \n",
       "\n",
       "   var_71_ratio  var_72_ratio  var_73_ratio  var_74_ratio  var_75_ratio  \\\n",
       "0      0.947633      1.017684      0.765609      1.161926      1.004696   \n",
       "1      0.927140      0.976393      1.066150      0.926612      0.852355   \n",
       "2      1.057728      1.072669      1.015130      1.011970      0.949724   \n",
       "3      0.787578      0.994356      0.861709      0.889981      1.069513   \n",
       "4      1.016399      1.033911      1.699006      0.767872      1.004326   \n",
       "\n",
       "   var_76_ratio  var_78_ratio  var_79_ratio  var_80_ratio  var_81_ratio  \\\n",
       "0      0.905047      1.036067      0.908603      1.137611      0.870138   \n",
       "1      0.988479      0.871881      0.931475      0.898107      1.052355   \n",
       "2      0.951553      1.013237      1.018472      0.994290      1.018201   \n",
       "3      1.011655      0.821320      1.073566      0.834580      2.291878   \n",
       "4      0.979263      1.171007      1.043875      0.947274      0.982413   \n",
       "\n",
       "   var_82_ratio  var_83_ratio  var_84_ratio  var_85_ratio  var_86_ratio  \\\n",
       "0      1.061127      1.009080      0.917883      0.941532      1.041782   \n",
       "1      1.037122      0.991057      0.913065      1.026026      0.000000   \n",
       "2      0.962919      0.984385      1.073277      0.999343      0.991762   \n",
       "3      1.072888      0.974842      1.103723      1.019714      0.944826   \n",
       "4      0.855327      0.961186      1.098946      1.065269      0.983013   \n",
       "\n",
       "   var_88_ratio  var_89_ratio  var_9_ratio  var_90_ratio  var_91_ratio  \\\n",
       "0      1.231490      0.696698     1.934524      0.967430      1.018207   \n",
       "1      1.229756      2.602401     0.923342      1.083392      0.962923   \n",
       "2      0.888863      0.894455     1.436729      0.965233      1.041557   \n",
       "3      1.015126      1.131716     1.033670      1.021356      1.006981   \n",
       "4      1.231490      0.999610     1.013785      0.674985      1.010778   \n",
       "\n",
       "   var_92_ratio  var_93_ratio  var_94_ratio  var_95_ratio  var_96_ratio  \\\n",
       "0      1.088268      0.877328      1.120142      1.030191      0.912910   \n",
       "1      0.891192      1.073670      0.999163      0.533389      1.105764   \n",
       "2      0.986033      0.919970      1.101490      1.054443      1.082102   \n",
       "3      0.964882      1.049607      0.892744      0.783332      0.939763   \n",
       "4      0.993879      0.924252      1.094295      0.997093      0.992454   \n",
       "\n",
       "   var_97_ratio  var_99_ratio  var_104_ratio  var_107_ratio  var_112_ratio  \\\n",
       "0      1.011907      1.018735       1.063925       1.015428       1.101427   \n",
       "1      0.971632      1.061841       1.058222       1.683168       1.187939   \n",
       "2      1.011907      1.022511       1.056645       0.790017       1.181691   \n",
       "3      0.979965      0.974896       0.939916       0.737724       0.841794   \n",
       "4      0.995211      1.061841       0.537980       0.917596       0.895950   \n",
       "\n",
       "   var_121_ratio  var_14_ratio  var_140_ratio  var_142_ratio  var_149_ratio  \\\n",
       "0       1.083466      0.998691       1.055831       1.173750       1.023453   \n",
       "1       1.005410      1.009463       0.976819       0.409186       0.941226   \n",
       "2       0.792797      1.004569       1.057686       1.018836       0.850479   \n",
       "3       1.015097      0.995473       0.988140       1.014332       0.863707   \n",
       "4       0.852852      0.997740       1.058544       1.029096       1.035228   \n",
       "\n",
       "   var_156_ratio  var_160_ratio  var_172_ratio  var_177_ratio  var_178_ratio  \\\n",
       "0       0.941332       1.194063       0.995053       0.868915       0.927647   \n",
       "1       0.956960       0.939387       1.003959       1.331807       0.981848   \n",
       "2       0.945035       1.135159       0.955382       0.821828       0.902246   \n",
       "3       0.918191       0.919336       0.992274       0.848997       0.891852   \n",
       "4       1.073733       0.900018       1.016959       0.891546       1.016351   \n",
       "\n",
       "   var_186_ratio  var_192_ratio  var_199_ratio  var_20_ratio  var_21_ratio  \\\n",
       "0       0.964357       1.067237       0.999461      1.016186      1.035990   \n",
       "1       1.048711       0.995883       1.111374      0.558398      1.069292   \n",
       "2       0.965016       1.004134       1.055433      0.979087      1.010823   \n",
       "3       1.036800       0.895976       0.883443      0.986104      0.952094   \n",
       "4       0.948608       0.856314       0.883443      0.992778      0.997854   \n",
       "\n",
       "   var_23_ratio  var_31_ratio  var_33_ratio  var_45_ratio  var_57_ratio  \\\n",
       "0      0.899440      1.026965      0.890931      1.006270      0.889903   \n",
       "1      0.952728      0.960437      0.866660      0.886594      0.988293   \n",
       "2      0.961735      1.004140      0.842703      1.006270      1.018511   \n",
       "3      1.026108      0.995137      0.749434      0.947081      1.020585   \n",
       "4      1.026108      1.013438      0.866660      0.000000      0.994185   \n",
       "\n",
       "   var_65_ratio  var_77_ratio  var_8_ratio  var_87_ratio  var_120_ratio  \\\n",
       "0      0.919103      0.987538     0.960883      0.965914       0.942483   \n",
       "1      0.992117      1.019159     1.135200      0.995041       1.048840   \n",
       "2      0.972071      0.987538     0.960883      1.028244       0.975031   \n",
       "3      1.006031      1.019159     1.340088      0.956568       1.032904   \n",
       "4      0.984521      1.006857     1.040710      1.077709       1.089686   \n",
       "\n",
       "   var_46_ratio  var_54_ratio  var_58_ratio  \n",
       "0      0.644645      0.928162      1.009814  \n",
       "1      0.955403      0.872108      1.009814  \n",
       "2      0.959222      1.105710      0.981893  \n",
       "3      1.054807      1.106460      1.023166  \n",
       "4      1.036233      0.857310      1.009814  \n",
       "\n",
       "[5 rows x 382 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "Test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "93aa6148650a23671d5f01a834f948c5e6721234"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>...</th>\n",
       "      <th>var_196_ratio</th>\n",
       "      <th>var_197_ratio</th>\n",
       "      <th>var_198_ratio</th>\n",
       "      <th>var_2_ratio</th>\n",
       "      <th>var_22_ratio</th>\n",
       "      <th>var_24_ratio</th>\n",
       "      <th>var_25_ratio</th>\n",
       "      <th>var_26_ratio</th>\n",
       "      <th>var_28_ratio</th>\n",
       "      <th>var_3_ratio</th>\n",
       "      <th>var_32_ratio</th>\n",
       "      <th>var_34_ratio</th>\n",
       "      <th>var_35_ratio</th>\n",
       "      <th>var_36_ratio</th>\n",
       "      <th>var_37_ratio</th>\n",
       "      <th>var_39_ratio</th>\n",
       "      <th>var_4_ratio</th>\n",
       "      <th>var_40_ratio</th>\n",
       "      <th>var_42_ratio</th>\n",
       "      <th>var_43_ratio</th>\n",
       "      <th>var_44_ratio</th>\n",
       "      <th>var_48_ratio</th>\n",
       "      <th>var_49_ratio</th>\n",
       "      <th>var_5_ratio</th>\n",
       "      <th>var_50_ratio</th>\n",
       "      <th>var_51_ratio</th>\n",
       "      <th>var_52_ratio</th>\n",
       "      <th>var_53_ratio</th>\n",
       "      <th>var_55_ratio</th>\n",
       "      <th>var_56_ratio</th>\n",
       "      <th>var_59_ratio</th>\n",
       "      <th>var_6_ratio</th>\n",
       "      <th>var_60_ratio</th>\n",
       "      <th>var_61_ratio</th>\n",
       "      <th>var_62_ratio</th>\n",
       "      <th>var_63_ratio</th>\n",
       "      <th>var_64_ratio</th>\n",
       "      <th>var_66_ratio</th>\n",
       "      <th>var_67_ratio</th>\n",
       "      <th>var_68_ratio</th>\n",
       "      <th>var_69_ratio</th>\n",
       "      <th>var_70_ratio</th>\n",
       "      <th>var_71_ratio</th>\n",
       "      <th>var_72_ratio</th>\n",
       "      <th>var_73_ratio</th>\n",
       "      <th>var_74_ratio</th>\n",
       "      <th>var_75_ratio</th>\n",
       "      <th>var_76_ratio</th>\n",
       "      <th>var_78_ratio</th>\n",
       "      <th>var_79_ratio</th>\n",
       "      <th>var_80_ratio</th>\n",
       "      <th>var_81_ratio</th>\n",
       "      <th>var_82_ratio</th>\n",
       "      <th>var_83_ratio</th>\n",
       "      <th>var_84_ratio</th>\n",
       "      <th>var_85_ratio</th>\n",
       "      <th>var_86_ratio</th>\n",
       "      <th>var_88_ratio</th>\n",
       "      <th>var_89_ratio</th>\n",
       "      <th>var_9_ratio</th>\n",
       "      <th>var_90_ratio</th>\n",
       "      <th>var_91_ratio</th>\n",
       "      <th>var_92_ratio</th>\n",
       "      <th>var_93_ratio</th>\n",
       "      <th>var_94_ratio</th>\n",
       "      <th>var_95_ratio</th>\n",
       "      <th>var_96_ratio</th>\n",
       "      <th>var_97_ratio</th>\n",
       "      <th>var_99_ratio</th>\n",
       "      <th>var_104_ratio</th>\n",
       "      <th>var_107_ratio</th>\n",
       "      <th>var_112_ratio</th>\n",
       "      <th>var_121_ratio</th>\n",
       "      <th>var_14_ratio</th>\n",
       "      <th>var_140_ratio</th>\n",
       "      <th>var_142_ratio</th>\n",
       "      <th>var_149_ratio</th>\n",
       "      <th>var_156_ratio</th>\n",
       "      <th>var_160_ratio</th>\n",
       "      <th>var_172_ratio</th>\n",
       "      <th>var_177_ratio</th>\n",
       "      <th>var_178_ratio</th>\n",
       "      <th>var_186_ratio</th>\n",
       "      <th>var_192_ratio</th>\n",
       "      <th>var_199_ratio</th>\n",
       "      <th>var_20_ratio</th>\n",
       "      <th>var_21_ratio</th>\n",
       "      <th>var_23_ratio</th>\n",
       "      <th>var_31_ratio</th>\n",
       "      <th>var_33_ratio</th>\n",
       "      <th>var_45_ratio</th>\n",
       "      <th>var_57_ratio</th>\n",
       "      <th>var_65_ratio</th>\n",
       "      <th>var_77_ratio</th>\n",
       "      <th>var_8_ratio</th>\n",
       "      <th>var_87_ratio</th>\n",
       "      <th>var_120_ratio</th>\n",
       "      <th>var_46_ratio</th>\n",
       "      <th>var_54_ratio</th>\n",
       "      <th>var_58_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>8.8100</td>\n",
       "      <td>-2.0248</td>\n",
       "      <td>-4.3554</td>\n",
       "      <td>13.9696</td>\n",
       "      <td>0.3458</td>\n",
       "      <td>7.5408</td>\n",
       "      <td>14.5001</td>\n",
       "      <td>7.7028</td>\n",
       "      <td>-19.0919</td>\n",
       "      <td>15.5806</td>\n",
       "      <td>16.1763</td>\n",
       "      <td>3.7088</td>\n",
       "      <td>18.8064</td>\n",
       "      <td>1.5899</td>\n",
       "      <td>3.0654</td>\n",
       "      <td>6.4509</td>\n",
       "      <td>14.1192</td>\n",
       "      <td>-9.4902</td>\n",
       "      <td>-2.1917</td>\n",
       "      <td>5.7107</td>\n",
       "      <td>3.7864</td>\n",
       "      <td>-1.7981</td>\n",
       "      <td>9.2645</td>\n",
       "      <td>2.0657</td>\n",
       "      <td>12.7753</td>\n",
       "      <td>11.3334</td>\n",
       "      <td>8.1462</td>\n",
       "      <td>-0.0610</td>\n",
       "      <td>3.5331</td>\n",
       "      <td>9.7804</td>\n",
       "      <td>8.7625</td>\n",
       "      <td>-15.6305</td>\n",
       "      <td>18.8766</td>\n",
       "      <td>11.2864</td>\n",
       "      <td>11.8362</td>\n",
       "      <td>13.3680</td>\n",
       "      <td>-31.9891</td>\n",
       "      <td>12.1776</td>\n",
       "      <td>8.7714</td>\n",
       "      <td>17.2011</td>\n",
       "      <td>16.8508</td>\n",
       "      <td>13.0534</td>\n",
       "      <td>14.4069</td>\n",
       "      <td>-4.8525</td>\n",
       "      <td>7.3213</td>\n",
       "      <td>-0.5259</td>\n",
       "      <td>16.6365</td>\n",
       "      <td>19.3036</td>\n",
       "      <td>6.4129</td>\n",
       "      <td>-5.3948</td>\n",
       "      <td>9.3269</td>\n",
       "      <td>11.9314</td>\n",
       "      <td>-3.5750</td>\n",
       "      <td>-0.7706</td>\n",
       "      <td>0.8705</td>\n",
       "      <td>6.9282</td>\n",
       "      <td>2.8914</td>\n",
       "      <td>5.9744</td>\n",
       "      <td>17.4851</td>\n",
       "      <td>5.0125</td>\n",
       "      <td>-1.4230</td>\n",
       "      <td>33.3401</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>-4.7906</td>\n",
       "      <td>30.2708</td>\n",
       "      <td>26.8339</td>\n",
       "      <td>21.7205</td>\n",
       "      <td>7.3075</td>\n",
       "      <td>14.0810</td>\n",
       "      <td>3.1192</td>\n",
       "      <td>17.4265</td>\n",
       "      <td>9.4883</td>\n",
       "      <td>16.9060</td>\n",
       "      <td>14.5117</td>\n",
       "      <td>10.0276</td>\n",
       "      <td>-0.9706</td>\n",
       "      <td>20.4588</td>\n",
       "      <td>4.7945</td>\n",
       "      <td>20.4160</td>\n",
       "      <td>13.1633</td>\n",
       "      <td>7.9307</td>\n",
       "      <td>-7.6509</td>\n",
       "      <td>7.0834</td>\n",
       "      <td>15.2324</td>\n",
       "      <td>10.1416</td>\n",
       "      <td>5.9156</td>\n",
       "      <td>-0.5775</td>\n",
       "      <td>5.7600</td>\n",
       "      <td>30.3238</td>\n",
       "      <td>2.1251</td>\n",
       "      <td>...</td>\n",
       "      <td>1.071594</td>\n",
       "      <td>1.153006</td>\n",
       "      <td>1.027325</td>\n",
       "      <td>1.075693</td>\n",
       "      <td>0.929048</td>\n",
       "      <td>1.102077</td>\n",
       "      <td>1.084514</td>\n",
       "      <td>0.877153</td>\n",
       "      <td>1.015963</td>\n",
       "      <td>1.104166</td>\n",
       "      <td>1.087910</td>\n",
       "      <td>0.984106</td>\n",
       "      <td>1.113751</td>\n",
       "      <td>0.929617</td>\n",
       "      <td>0.923175</td>\n",
       "      <td>0.846942</td>\n",
       "      <td>1.036313</td>\n",
       "      <td>0.955849</td>\n",
       "      <td>0.997286</td>\n",
       "      <td>0.872239</td>\n",
       "      <td>0.991373</td>\n",
       "      <td>0.972361</td>\n",
       "      <td>0.986368</td>\n",
       "      <td>1.003991</td>\n",
       "      <td>1.000125</td>\n",
       "      <td>0.964512</td>\n",
       "      <td>0.953046</td>\n",
       "      <td>1.148224</td>\n",
       "      <td>1.027804</td>\n",
       "      <td>0.949831</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.978014</td>\n",
       "      <td>1.022414</td>\n",
       "      <td>1.083533</td>\n",
       "      <td>1.011738</td>\n",
       "      <td>1.042202</td>\n",
       "      <td>0.967092</td>\n",
       "      <td>0.959436</td>\n",
       "      <td>1.025783</td>\n",
       "      <td>0.880721</td>\n",
       "      <td>1.113630</td>\n",
       "      <td>1.003394</td>\n",
       "      <td>1.055261</td>\n",
       "      <td>1.002495</td>\n",
       "      <td>1.027508</td>\n",
       "      <td>1.014519</td>\n",
       "      <td>0.927862</td>\n",
       "      <td>0.968576</td>\n",
       "      <td>1.071162</td>\n",
       "      <td>2.951772</td>\n",
       "      <td>0.988266</td>\n",
       "      <td>1.093841</td>\n",
       "      <td>1.541832</td>\n",
       "      <td>1.025807</td>\n",
       "      <td>1.008037</td>\n",
       "      <td>0.926751</td>\n",
       "      <td>0.924111</td>\n",
       "      <td>1.150322</td>\n",
       "      <td>1.141009</td>\n",
       "      <td>1.026816</td>\n",
       "      <td>1.036020</td>\n",
       "      <td>1.017468</td>\n",
       "      <td>0.957805</td>\n",
       "      <td>0.899621</td>\n",
       "      <td>0.592629</td>\n",
       "      <td>0.955423</td>\n",
       "      <td>0.884545</td>\n",
       "      <td>1.003864</td>\n",
       "      <td>0.981864</td>\n",
       "      <td>1.018372</td>\n",
       "      <td>0.986088</td>\n",
       "      <td>1.166043</td>\n",
       "      <td>1.004279</td>\n",
       "      <td>1.004569</td>\n",
       "      <td>2.899864</td>\n",
       "      <td>1.173750</td>\n",
       "      <td>0.969311</td>\n",
       "      <td>0.885866</td>\n",
       "      <td>1.057211</td>\n",
       "      <td>1.045474</td>\n",
       "      <td>1.312693</td>\n",
       "      <td>0.960050</td>\n",
       "      <td>1.022345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.876353</td>\n",
       "      <td>0.318066</td>\n",
       "      <td>0.972321</td>\n",
       "      <td>1.036051</td>\n",
       "      <td>1.001298</td>\n",
       "      <td>1.200054</td>\n",
       "      <td>0.945387</td>\n",
       "      <td>0.996193</td>\n",
       "      <td>1.002181</td>\n",
       "      <td>0.897869</td>\n",
       "      <td>1.086287</td>\n",
       "      <td>1.085689</td>\n",
       "      <td>1.135204</td>\n",
       "      <td>1.033577</td>\n",
       "      <td>0.921647</td>\n",
       "      <td>1.044576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>5.9739</td>\n",
       "      <td>-1.3809</td>\n",
       "      <td>-0.3310</td>\n",
       "      <td>14.1129</td>\n",
       "      <td>2.5667</td>\n",
       "      <td>5.4988</td>\n",
       "      <td>14.1853</td>\n",
       "      <td>7.0196</td>\n",
       "      <td>4.6564</td>\n",
       "      <td>29.1609</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>12.1469</td>\n",
       "      <td>3.1389</td>\n",
       "      <td>5.2578</td>\n",
       "      <td>2.4228</td>\n",
       "      <td>16.2064</td>\n",
       "      <td>13.5023</td>\n",
       "      <td>-5.2341</td>\n",
       "      <td>-3.6648</td>\n",
       "      <td>5.7080</td>\n",
       "      <td>2.9965</td>\n",
       "      <td>-10.4720</td>\n",
       "      <td>11.4938</td>\n",
       "      <td>-0.9660</td>\n",
       "      <td>15.3445</td>\n",
       "      <td>10.6361</td>\n",
       "      <td>0.8966</td>\n",
       "      <td>6.7428</td>\n",
       "      <td>2.3421</td>\n",
       "      <td>12.8678</td>\n",
       "      <td>-1.5536</td>\n",
       "      <td>10.0309</td>\n",
       "      <td>3.1337</td>\n",
       "      <td>10.5742</td>\n",
       "      <td>11.7664</td>\n",
       "      <td>2.1782</td>\n",
       "      <td>-41.1924</td>\n",
       "      <td>13.5322</td>\n",
       "      <td>-17.3834</td>\n",
       "      <td>6.3806</td>\n",
       "      <td>12.5589</td>\n",
       "      <td>11.6887</td>\n",
       "      <td>25.3930</td>\n",
       "      <td>1.5776</td>\n",
       "      <td>6.8481</td>\n",
       "      <td>8.7348</td>\n",
       "      <td>16.4239</td>\n",
       "      <td>21.7056</td>\n",
       "      <td>6.9345</td>\n",
       "      <td>1.6678</td>\n",
       "      <td>9.5249</td>\n",
       "      <td>5.3383</td>\n",
       "      <td>-18.7083</td>\n",
       "      <td>1.3382</td>\n",
       "      <td>-1.7401</td>\n",
       "      <td>5.8398</td>\n",
       "      <td>3.1051</td>\n",
       "      <td>4.4307</td>\n",
       "      <td>16.0005</td>\n",
       "      <td>5.0306</td>\n",
       "      <td>-7.3365</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>0.6992</td>\n",
       "      <td>-0.7772</td>\n",
       "      <td>21.5123</td>\n",
       "      <td>6.7803</td>\n",
       "      <td>18.1896</td>\n",
       "      <td>6.9388</td>\n",
       "      <td>22.1336</td>\n",
       "      <td>6.3755</td>\n",
       "      <td>13.1525</td>\n",
       "      <td>1.9772</td>\n",
       "      <td>14.0406</td>\n",
       "      <td>6.6904</td>\n",
       "      <td>9.9732</td>\n",
       "      <td>-11.5679</td>\n",
       "      <td>20.4525</td>\n",
       "      <td>9.4951</td>\n",
       "      <td>9.6343</td>\n",
       "      <td>8.1252</td>\n",
       "      <td>2.6059</td>\n",
       "      <td>-17.4201</td>\n",
       "      <td>7.1848</td>\n",
       "      <td>15.3484</td>\n",
       "      <td>10.6522</td>\n",
       "      <td>5.9897</td>\n",
       "      <td>0.3392</td>\n",
       "      <td>10.3516</td>\n",
       "      <td>29.8204</td>\n",
       "      <td>1.9998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859378</td>\n",
       "      <td>0.844767</td>\n",
       "      <td>1.021946</td>\n",
       "      <td>0.962096</td>\n",
       "      <td>1.063508</td>\n",
       "      <td>1.360240</td>\n",
       "      <td>0.948709</td>\n",
       "      <td>1.003233</td>\n",
       "      <td>1.015963</td>\n",
       "      <td>0.902046</td>\n",
       "      <td>0.961710</td>\n",
       "      <td>0.942953</td>\n",
       "      <td>0.902421</td>\n",
       "      <td>1.088766</td>\n",
       "      <td>1.015836</td>\n",
       "      <td>0.912505</td>\n",
       "      <td>0.941976</td>\n",
       "      <td>1.245700</td>\n",
       "      <td>0.863423</td>\n",
       "      <td>0.912171</td>\n",
       "      <td>1.403513</td>\n",
       "      <td>0.992077</td>\n",
       "      <td>0.930214</td>\n",
       "      <td>0.954073</td>\n",
       "      <td>1.278785</td>\n",
       "      <td>0.979607</td>\n",
       "      <td>1.067269</td>\n",
       "      <td>1.172779</td>\n",
       "      <td>1.027804</td>\n",
       "      <td>1.073180</td>\n",
       "      <td>0.822529</td>\n",
       "      <td>1.056568</td>\n",
       "      <td>0.571836</td>\n",
       "      <td>0.916041</td>\n",
       "      <td>0.965992</td>\n",
       "      <td>0.970648</td>\n",
       "      <td>1.032988</td>\n",
       "      <td>0.768972</td>\n",
       "      <td>0.986626</td>\n",
       "      <td>1.142129</td>\n",
       "      <td>0.985562</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>1.003287</td>\n",
       "      <td>1.033226</td>\n",
       "      <td>1.014497</td>\n",
       "      <td>0.874244</td>\n",
       "      <td>0.995326</td>\n",
       "      <td>0.965819</td>\n",
       "      <td>1.036067</td>\n",
       "      <td>1.027893</td>\n",
       "      <td>0.898107</td>\n",
       "      <td>0.870138</td>\n",
       "      <td>1.161277</td>\n",
       "      <td>1.009023</td>\n",
       "      <td>0.886786</td>\n",
       "      <td>0.926751</td>\n",
       "      <td>1.082121</td>\n",
       "      <td>1.231490</td>\n",
       "      <td>0.893753</td>\n",
       "      <td>1.642776</td>\n",
       "      <td>0.983993</td>\n",
       "      <td>1.000640</td>\n",
       "      <td>0.958733</td>\n",
       "      <td>0.991959</td>\n",
       "      <td>0.644963</td>\n",
       "      <td>1.030191</td>\n",
       "      <td>0.910490</td>\n",
       "      <td>1.003864</td>\n",
       "      <td>0.949477</td>\n",
       "      <td>1.017850</td>\n",
       "      <td>0.634520</td>\n",
       "      <td>1.049403</td>\n",
       "      <td>0.968906</td>\n",
       "      <td>1.009463</td>\n",
       "      <td>1.011640</td>\n",
       "      <td>0.735335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.141756</td>\n",
       "      <td>0.957236</td>\n",
       "      <td>1.075419</td>\n",
       "      <td>0.656129</td>\n",
       "      <td>0.940605</td>\n",
       "      <td>0.953552</td>\n",
       "      <td>0.904601</td>\n",
       "      <td>2.066700</td>\n",
       "      <td>1.045679</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>0.838075</td>\n",
       "      <td>1.003727</td>\n",
       "      <td>0.991604</td>\n",
       "      <td>0.976678</td>\n",
       "      <td>1.001069</td>\n",
       "      <td>1.007946</td>\n",
       "      <td>0.956957</td>\n",
       "      <td>0.972639</td>\n",
       "      <td>1.041837</td>\n",
       "      <td>0.940335</td>\n",
       "      <td>1.050002</td>\n",
       "      <td>1.098613</td>\n",
       "      <td>0.967202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>8.3442</td>\n",
       "      <td>-4.7057</td>\n",
       "      <td>-3.0422</td>\n",
       "      <td>13.6751</td>\n",
       "      <td>3.8183</td>\n",
       "      <td>10.8535</td>\n",
       "      <td>14.2126</td>\n",
       "      <td>9.8837</td>\n",
       "      <td>2.6541</td>\n",
       "      <td>21.2181</td>\n",
       "      <td>20.8163</td>\n",
       "      <td>12.4666</td>\n",
       "      <td>12.3696</td>\n",
       "      <td>4.7473</td>\n",
       "      <td>2.7936</td>\n",
       "      <td>5.2189</td>\n",
       "      <td>13.5670</td>\n",
       "      <td>-15.4246</td>\n",
       "      <td>-0.1655</td>\n",
       "      <td>7.2633</td>\n",
       "      <td>3.4310</td>\n",
       "      <td>-9.1508</td>\n",
       "      <td>9.7320</td>\n",
       "      <td>3.1062</td>\n",
       "      <td>22.3076</td>\n",
       "      <td>11.9593</td>\n",
       "      <td>9.9255</td>\n",
       "      <td>4.0702</td>\n",
       "      <td>4.9934</td>\n",
       "      <td>8.0667</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>-19.0841</td>\n",
       "      <td>5.2272</td>\n",
       "      <td>9.5977</td>\n",
       "      <td>12.1801</td>\n",
       "      <td>8.3565</td>\n",
       "      <td>15.1170</td>\n",
       "      <td>10.0921</td>\n",
       "      <td>-20.8504</td>\n",
       "      <td>8.6758</td>\n",
       "      <td>8.1292</td>\n",
       "      <td>11.8932</td>\n",
       "      <td>10.6869</td>\n",
       "      <td>-0.6434</td>\n",
       "      <td>5.6510</td>\n",
       "      <td>9.3742</td>\n",
       "      <td>25.8831</td>\n",
       "      <td>19.8701</td>\n",
       "      <td>5.4834</td>\n",
       "      <td>-4.0304</td>\n",
       "      <td>8.5160</td>\n",
       "      <td>8.9776</td>\n",
       "      <td>-5.6619</td>\n",
       "      <td>2.8117</td>\n",
       "      <td>2.5996</td>\n",
       "      <td>9.0986</td>\n",
       "      <td>7.1167</td>\n",
       "      <td>4.9466</td>\n",
       "      <td>13.8268</td>\n",
       "      <td>5.0093</td>\n",
       "      <td>4.7782</td>\n",
       "      <td>19.2081</td>\n",
       "      <td>0.4340</td>\n",
       "      <td>0.8459</td>\n",
       "      <td>34.8598</td>\n",
       "      <td>20.7048</td>\n",
       "      <td>16.4953</td>\n",
       "      <td>-9.7077</td>\n",
       "      <td>19.6357</td>\n",
       "      <td>7.6587</td>\n",
       "      <td>15.5744</td>\n",
       "      <td>16.1691</td>\n",
       "      <td>14.3299</td>\n",
       "      <td>1.3360</td>\n",
       "      <td>-0.4412</td>\n",
       "      <td>-0.2830</td>\n",
       "      <td>14.9105</td>\n",
       "      <td>-3.9016</td>\n",
       "      <td>14.6881</td>\n",
       "      <td>7.3220</td>\n",
       "      <td>-5.1443</td>\n",
       "      <td>-34.3488</td>\n",
       "      <td>7.0194</td>\n",
       "      <td>12.4785</td>\n",
       "      <td>9.6665</td>\n",
       "      <td>13.2595</td>\n",
       "      <td>-0.5624</td>\n",
       "      <td>5.6347</td>\n",
       "      <td>9.5853</td>\n",
       "      <td>1.4515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950892</td>\n",
       "      <td>3.092792</td>\n",
       "      <td>1.103173</td>\n",
       "      <td>1.094811</td>\n",
       "      <td>1.019312</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.943052</td>\n",
       "      <td>0.698982</td>\n",
       "      <td>0.881027</td>\n",
       "      <td>0.969969</td>\n",
       "      <td>1.087236</td>\n",
       "      <td>0.960473</td>\n",
       "      <td>1.134192</td>\n",
       "      <td>0.974184</td>\n",
       "      <td>0.868174</td>\n",
       "      <td>1.058375</td>\n",
       "      <td>0.851617</td>\n",
       "      <td>1.049897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.027443</td>\n",
       "      <td>0.984616</td>\n",
       "      <td>1.013983</td>\n",
       "      <td>1.017188</td>\n",
       "      <td>1.030596</td>\n",
       "      <td>0.996007</td>\n",
       "      <td>0.937493</td>\n",
       "      <td>0.933610</td>\n",
       "      <td>0.816736</td>\n",
       "      <td>3.918750</td>\n",
       "      <td>0.948736</td>\n",
       "      <td>1.223166</td>\n",
       "      <td>0.951430</td>\n",
       "      <td>0.990746</td>\n",
       "      <td>1.069046</td>\n",
       "      <td>0.990688</td>\n",
       "      <td>0.960151</td>\n",
       "      <td>0.976385</td>\n",
       "      <td>0.967642</td>\n",
       "      <td>0.962138</td>\n",
       "      <td>1.032320</td>\n",
       "      <td>2.640762</td>\n",
       "      <td>1.001662</td>\n",
       "      <td>0.926602</td>\n",
       "      <td>0.985500</td>\n",
       "      <td>1.024586</td>\n",
       "      <td>0.954614</td>\n",
       "      <td>1.069513</td>\n",
       "      <td>2.809890</td>\n",
       "      <td>1.278505</td>\n",
       "      <td>1.065451</td>\n",
       "      <td>1.230617</td>\n",
       "      <td>0.893605</td>\n",
       "      <td>1.072888</td>\n",
       "      <td>1.033928</td>\n",
       "      <td>1.036610</td>\n",
       "      <td>1.135605</td>\n",
       "      <td>0.973719</td>\n",
       "      <td>1.161012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.086410</td>\n",
       "      <td>0.887029</td>\n",
       "      <td>1.010778</td>\n",
       "      <td>1.059650</td>\n",
       "      <td>1.348678</td>\n",
       "      <td>1.081930</td>\n",
       "      <td>0.955423</td>\n",
       "      <td>0.884545</td>\n",
       "      <td>0.962684</td>\n",
       "      <td>0.997798</td>\n",
       "      <td>0.983745</td>\n",
       "      <td>1.120921</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.868519</td>\n",
       "      <td>1.076684</td>\n",
       "      <td>2.187513</td>\n",
       "      <td>0.725777</td>\n",
       "      <td>0.977746</td>\n",
       "      <td>1.068856</td>\n",
       "      <td>0.841795</td>\n",
       "      <td>1.005458</td>\n",
       "      <td>1.042237</td>\n",
       "      <td>0.915866</td>\n",
       "      <td>1.064954</td>\n",
       "      <td>1.003664</td>\n",
       "      <td>3.109997</td>\n",
       "      <td>1.031274</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.997614</td>\n",
       "      <td>1.003424</td>\n",
       "      <td>1.400196</td>\n",
       "      <td>1.036542</td>\n",
       "      <td>0.898564</td>\n",
       "      <td>1.006031</td>\n",
       "      <td>1.006857</td>\n",
       "      <td>1.052565</td>\n",
       "      <td>0.993483</td>\n",
       "      <td>1.039746</td>\n",
       "      <td>0.934629</td>\n",
       "      <td>1.098764</td>\n",
       "      <td>1.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>7.4578</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>-5.0659</td>\n",
       "      <td>14.0526</td>\n",
       "      <td>13.5010</td>\n",
       "      <td>8.7660</td>\n",
       "      <td>14.7352</td>\n",
       "      <td>10.0383</td>\n",
       "      <td>-15.3508</td>\n",
       "      <td>2.1273</td>\n",
       "      <td>21.4797</td>\n",
       "      <td>14.5372</td>\n",
       "      <td>12.5527</td>\n",
       "      <td>2.9707</td>\n",
       "      <td>4.2398</td>\n",
       "      <td>13.7796</td>\n",
       "      <td>14.1408</td>\n",
       "      <td>1.0061</td>\n",
       "      <td>-1.3479</td>\n",
       "      <td>5.2570</td>\n",
       "      <td>6.5911</td>\n",
       "      <td>6.2161</td>\n",
       "      <td>9.5540</td>\n",
       "      <td>2.3628</td>\n",
       "      <td>10.2124</td>\n",
       "      <td>10.8047</td>\n",
       "      <td>-2.5588</td>\n",
       "      <td>6.0720</td>\n",
       "      <td>3.2613</td>\n",
       "      <td>16.5632</td>\n",
       "      <td>8.8336</td>\n",
       "      <td>-4.8327</td>\n",
       "      <td>0.9554</td>\n",
       "      <td>12.3754</td>\n",
       "      <td>11.4241</td>\n",
       "      <td>6.6917</td>\n",
       "      <td>-12.9761</td>\n",
       "      <td>13.7343</td>\n",
       "      <td>5.0150</td>\n",
       "      <td>31.3923</td>\n",
       "      <td>5.8555</td>\n",
       "      <td>12.6082</td>\n",
       "      <td>1.4182</td>\n",
       "      <td>-4.1185</td>\n",
       "      <td>6.2536</td>\n",
       "      <td>1.4257</td>\n",
       "      <td>13.5426</td>\n",
       "      <td>15.4090</td>\n",
       "      <td>6.8761</td>\n",
       "      <td>1.7476</td>\n",
       "      <td>10.0413</td>\n",
       "      <td>15.2857</td>\n",
       "      <td>-4.1378</td>\n",
       "      <td>0.7928</td>\n",
       "      <td>2.5301</td>\n",
       "      <td>8.1458</td>\n",
       "      <td>2.5738</td>\n",
       "      <td>5.9876</td>\n",
       "      <td>13.0758</td>\n",
       "      <td>5.0087</td>\n",
       "      <td>-9.7824</td>\n",
       "      <td>8.9289</td>\n",
       "      <td>0.4205</td>\n",
       "      <td>-2.5463</td>\n",
       "      <td>2.9428</td>\n",
       "      <td>10.7087</td>\n",
       "      <td>12.2008</td>\n",
       "      <td>12.5465</td>\n",
       "      <td>19.4201</td>\n",
       "      <td>5.5060</td>\n",
       "      <td>14.1586</td>\n",
       "      <td>17.5941</td>\n",
       "      <td>15.4375</td>\n",
       "      <td>-13.2668</td>\n",
       "      <td>14.0885</td>\n",
       "      <td>4.0357</td>\n",
       "      <td>22.3119</td>\n",
       "      <td>1.8571</td>\n",
       "      <td>16.5210</td>\n",
       "      <td>10.8149</td>\n",
       "      <td>0.3256</td>\n",
       "      <td>-21.4797</td>\n",
       "      <td>6.9174</td>\n",
       "      <td>9.9483</td>\n",
       "      <td>10.3696</td>\n",
       "      <td>11.0362</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>19.4321</td>\n",
       "      <td>40.3383</td>\n",
       "      <td>1.4105</td>\n",
       "      <td>...</td>\n",
       "      <td>1.036400</td>\n",
       "      <td>1.047166</td>\n",
       "      <td>0.835253</td>\n",
       "      <td>0.888970</td>\n",
       "      <td>0.933459</td>\n",
       "      <td>0.892451</td>\n",
       "      <td>1.148161</td>\n",
       "      <td>1.112962</td>\n",
       "      <td>0.914898</td>\n",
       "      <td>0.949230</td>\n",
       "      <td>1.111000</td>\n",
       "      <td>0.981641</td>\n",
       "      <td>0.869066</td>\n",
       "      <td>1.086045</td>\n",
       "      <td>0.980036</td>\n",
       "      <td>0.846942</td>\n",
       "      <td>0.927008</td>\n",
       "      <td>1.101379</td>\n",
       "      <td>1.018886</td>\n",
       "      <td>1.150795</td>\n",
       "      <td>0.990496</td>\n",
       "      <td>1.146809</td>\n",
       "      <td>0.949642</td>\n",
       "      <td>1.255122</td>\n",
       "      <td>1.072535</td>\n",
       "      <td>0.901461</td>\n",
       "      <td>0.994539</td>\n",
       "      <td>1.118987</td>\n",
       "      <td>1.023795</td>\n",
       "      <td>1.058432</td>\n",
       "      <td>0.886971</td>\n",
       "      <td>0.936735</td>\n",
       "      <td>1.009341</td>\n",
       "      <td>1.083533</td>\n",
       "      <td>0.993678</td>\n",
       "      <td>0.960151</td>\n",
       "      <td>1.010157</td>\n",
       "      <td>0.959436</td>\n",
       "      <td>0.981961</td>\n",
       "      <td>1.203472</td>\n",
       "      <td>0.836357</td>\n",
       "      <td>0.900457</td>\n",
       "      <td>0.918880</td>\n",
       "      <td>0.975406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.889981</td>\n",
       "      <td>0.947440</td>\n",
       "      <td>1.011655</td>\n",
       "      <td>0.871881</td>\n",
       "      <td>0.944015</td>\n",
       "      <td>1.089865</td>\n",
       "      <td>0.987152</td>\n",
       "      <td>0.899392</td>\n",
       "      <td>1.165655</td>\n",
       "      <td>1.110395</td>\n",
       "      <td>0.938730</td>\n",
       "      <td>0.947992</td>\n",
       "      <td>0.813170</td>\n",
       "      <td>0.904958</td>\n",
       "      <td>0.973884</td>\n",
       "      <td>0.967430</td>\n",
       "      <td>0.946355</td>\n",
       "      <td>1.008301</td>\n",
       "      <td>0.931385</td>\n",
       "      <td>1.015048</td>\n",
       "      <td>0.997093</td>\n",
       "      <td>1.023441</td>\n",
       "      <td>0.995909</td>\n",
       "      <td>0.981609</td>\n",
       "      <td>1.013304</td>\n",
       "      <td>0.963549</td>\n",
       "      <td>1.186202</td>\n",
       "      <td>1.178879</td>\n",
       "      <td>0.998691</td>\n",
       "      <td>0.993216</td>\n",
       "      <td>0.885354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.983195</td>\n",
       "      <td>1.134118</td>\n",
       "      <td>0.920478</td>\n",
       "      <td>0.815710</td>\n",
       "      <td>1.036581</td>\n",
       "      <td>0.705848</td>\n",
       "      <td>1.000832</td>\n",
       "      <td>0.923711</td>\n",
       "      <td>1.009285</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>1.221038</td>\n",
       "      <td>1.013438</td>\n",
       "      <td>1.003302</td>\n",
       "      <td>0.945524</td>\n",
       "      <td>1.005237</td>\n",
       "      <td>0.997823</td>\n",
       "      <td>1.010598</td>\n",
       "      <td>1.136648</td>\n",
       "      <td>0.965914</td>\n",
       "      <td>0.968145</td>\n",
       "      <td>1.036233</td>\n",
       "      <td>0.972784</td>\n",
       "      <td>0.967202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>7.1437</td>\n",
       "      <td>5.1025</td>\n",
       "      <td>-3.2827</td>\n",
       "      <td>14.1013</td>\n",
       "      <td>8.9672</td>\n",
       "      <td>4.7276</td>\n",
       "      <td>14.5811</td>\n",
       "      <td>11.8615</td>\n",
       "      <td>3.1480</td>\n",
       "      <td>18.0126</td>\n",
       "      <td>13.8006</td>\n",
       "      <td>1.6026</td>\n",
       "      <td>16.3059</td>\n",
       "      <td>6.7954</td>\n",
       "      <td>3.6015</td>\n",
       "      <td>13.6569</td>\n",
       "      <td>13.8807</td>\n",
       "      <td>8.6228</td>\n",
       "      <td>-2.2654</td>\n",
       "      <td>5.2255</td>\n",
       "      <td>7.0165</td>\n",
       "      <td>-15.6961</td>\n",
       "      <td>10.6239</td>\n",
       "      <td>-4.7674</td>\n",
       "      <td>17.5447</td>\n",
       "      <td>11.8668</td>\n",
       "      <td>3.0154</td>\n",
       "      <td>4.2546</td>\n",
       "      <td>6.7601</td>\n",
       "      <td>5.9613</td>\n",
       "      <td>0.3695</td>\n",
       "      <td>-14.4364</td>\n",
       "      <td>5.1392</td>\n",
       "      <td>11.6336</td>\n",
       "      <td>12.0338</td>\n",
       "      <td>18.9670</td>\n",
       "      <td>12.0144</td>\n",
       "      <td>16.2096</td>\n",
       "      <td>-2.1966</td>\n",
       "      <td>1.1174</td>\n",
       "      <td>13.4532</td>\n",
       "      <td>12.7925</td>\n",
       "      <td>4.3775</td>\n",
       "      <td>-0.1543</td>\n",
       "      <td>5.6794</td>\n",
       "      <td>0.8210</td>\n",
       "      <td>19.1358</td>\n",
       "      <td>12.6589</td>\n",
       "      <td>6.4394</td>\n",
       "      <td>4.3425</td>\n",
       "      <td>8.7003</td>\n",
       "      <td>12.0586</td>\n",
       "      <td>-10.4753</td>\n",
       "      <td>-0.0337</td>\n",
       "      <td>5.6603</td>\n",
       "      <td>6.2529</td>\n",
       "      <td>1.5238</td>\n",
       "      <td>4.5356</td>\n",
       "      <td>20.1344</td>\n",
       "      <td>5.0267</td>\n",
       "      <td>-1.8628</td>\n",
       "      <td>39.8219</td>\n",
       "      <td>1.0498</td>\n",
       "      <td>-0.9113</td>\n",
       "      <td>38.5076</td>\n",
       "      <td>2.2201</td>\n",
       "      <td>9.5235</td>\n",
       "      <td>8.1522</td>\n",
       "      <td>14.9224</td>\n",
       "      <td>6.1573</td>\n",
       "      <td>15.5221</td>\n",
       "      <td>11.8133</td>\n",
       "      <td>16.7661</td>\n",
       "      <td>-14.6524</td>\n",
       "      <td>-0.4469</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>22.5276</td>\n",
       "      <td>6.9774</td>\n",
       "      <td>2.2563</td>\n",
       "      <td>3.5779</td>\n",
       "      <td>1.4268</td>\n",
       "      <td>9.0680</td>\n",
       "      <td>7.0197</td>\n",
       "      <td>19.7765</td>\n",
       "      <td>10.0499</td>\n",
       "      <td>11.4803</td>\n",
       "      <td>0.2548</td>\n",
       "      <td>16.7029</td>\n",
       "      <td>45.5510</td>\n",
       "      <td>1.5795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876416</td>\n",
       "      <td>1.936247</td>\n",
       "      <td>0.922273</td>\n",
       "      <td>1.072389</td>\n",
       "      <td>1.048108</td>\n",
       "      <td>0.891793</td>\n",
       "      <td>1.052948</td>\n",
       "      <td>6.579071</td>\n",
       "      <td>0.901573</td>\n",
       "      <td>1.035624</td>\n",
       "      <td>0.627534</td>\n",
       "      <td>0.934480</td>\n",
       "      <td>0.882066</td>\n",
       "      <td>0.996479</td>\n",
       "      <td>1.066665</td>\n",
       "      <td>1.042880</td>\n",
       "      <td>0.933657</td>\n",
       "      <td>0.934741</td>\n",
       "      <td>1.031936</td>\n",
       "      <td>0.988943</td>\n",
       "      <td>0.347748</td>\n",
       "      <td>0.926828</td>\n",
       "      <td>0.955335</td>\n",
       "      <td>0.792603</td>\n",
       "      <td>1.040895</td>\n",
       "      <td>1.033227</td>\n",
       "      <td>0.955693</td>\n",
       "      <td>0.816736</td>\n",
       "      <td>0.978541</td>\n",
       "      <td>0.961420</td>\n",
       "      <td>1.225596</td>\n",
       "      <td>1.208582</td>\n",
       "      <td>1.008235</td>\n",
       "      <td>1.004433</td>\n",
       "      <td>1.017591</td>\n",
       "      <td>1.056296</td>\n",
       "      <td>1.049591</td>\n",
       "      <td>0.853070</td>\n",
       "      <td>1.018370</td>\n",
       "      <td>1.048746</td>\n",
       "      <td>1.111200</td>\n",
       "      <td>1.030801</td>\n",
       "      <td>1.174515</td>\n",
       "      <td>1.033226</td>\n",
       "      <td>1.024075</td>\n",
       "      <td>0.812347</td>\n",
       "      <td>0.806520</td>\n",
       "      <td>0.990602</td>\n",
       "      <td>0.986935</td>\n",
       "      <td>1.065451</td>\n",
       "      <td>1.113453</td>\n",
       "      <td>1.086079</td>\n",
       "      <td>1.017064</td>\n",
       "      <td>1.033928</td>\n",
       "      <td>1.071710</td>\n",
       "      <td>0.954951</td>\n",
       "      <td>0.993734</td>\n",
       "      <td>2.300122</td>\n",
       "      <td>0.876417</td>\n",
       "      <td>0.861295</td>\n",
       "      <td>1.706445</td>\n",
       "      <td>1.010778</td>\n",
       "      <td>1.058171</td>\n",
       "      <td>0.897156</td>\n",
       "      <td>0.959945</td>\n",
       "      <td>0.997693</td>\n",
       "      <td>0.997531</td>\n",
       "      <td>1.069630</td>\n",
       "      <td>1.022511</td>\n",
       "      <td>1.053644</td>\n",
       "      <td>0.993833</td>\n",
       "      <td>0.867665</td>\n",
       "      <td>1.015097</td>\n",
       "      <td>0.928777</td>\n",
       "      <td>1.009986</td>\n",
       "      <td>1.377834</td>\n",
       "      <td>1.172329</td>\n",
       "      <td>1.044696</td>\n",
       "      <td>1.087812</td>\n",
       "      <td>0.992274</td>\n",
       "      <td>1.307596</td>\n",
       "      <td>0.936262</td>\n",
       "      <td>1.033690</td>\n",
       "      <td>0.943855</td>\n",
       "      <td>0.876353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.028467</td>\n",
       "      <td>0.956553</td>\n",
       "      <td>0.997273</td>\n",
       "      <td>0.833296</td>\n",
       "      <td>1.057838</td>\n",
       "      <td>1.003821</td>\n",
       "      <td>1.008596</td>\n",
       "      <td>0.916172</td>\n",
       "      <td>1.135200</td>\n",
       "      <td>0.960790</td>\n",
       "      <td>1.100778</td>\n",
       "      <td>1.046678</td>\n",
       "      <td>0.966339</td>\n",
       "      <td>1.023166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 381 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    var_0    var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  test_0  11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493   \n",
       "1  test_1   8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196   \n",
       "2  test_2   5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950   \n",
       "3  test_3   8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397   \n",
       "4  test_4  11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595   \n",
       "\n",
       "     var_7   var_8   var_9  var_10  var_11   var_12   var_13   var_14  \\\n",
       "0  18.2675  2.1337  8.8100 -2.0248 -4.3554  13.9696   0.3458   7.5408   \n",
       "1  18.6316 -4.4131  5.9739 -1.3809 -0.3310  14.1129   2.5667   5.4988   \n",
       "2  20.2537  1.5233  8.3442 -4.7057 -3.0422  13.6751   3.8183  10.8535   \n",
       "3  20.5660  3.3755  7.4578  0.0095 -5.0659  14.0526  13.5010   8.7660   \n",
       "4  10.6048  2.9890  7.1437  5.1025 -3.2827  14.1013   8.9672   4.7276   \n",
       "\n",
       "    var_15   var_16   var_17   var_18   var_19   var_20   var_21  var_22  \\\n",
       "0  14.5001   7.7028 -19.0919  15.5806  16.1763   3.7088  18.8064  1.5899   \n",
       "1  14.1853   7.0196   4.6564  29.1609   0.0910  12.1469   3.1389  5.2578   \n",
       "2  14.2126   9.8837   2.6541  21.2181  20.8163  12.4666  12.3696  4.7473   \n",
       "3  14.7352  10.0383 -15.3508   2.1273  21.4797  14.5372  12.5527  2.9707   \n",
       "4  14.5811  11.8615   3.1480  18.0126  13.8006   1.6026  16.3059  6.7954   \n",
       "\n",
       "   var_23   var_24   var_25   var_26  var_27  var_28  var_29   var_30  \\\n",
       "0  3.0654   6.4509  14.1192  -9.4902 -2.1917  5.7107  3.7864  -1.7981   \n",
       "1  2.4228  16.2064  13.5023  -5.2341 -3.6648  5.7080  2.9965 -10.4720   \n",
       "2  2.7936   5.2189  13.5670 -15.4246 -0.1655  7.2633  3.4310  -9.1508   \n",
       "3  4.2398  13.7796  14.1408   1.0061 -1.3479  5.2570  6.5911   6.2161   \n",
       "4  3.6015  13.6569  13.8807   8.6228 -2.2654  5.2255  7.0165 -15.6961   \n",
       "\n",
       "    var_31  var_32   var_33   var_34  var_35  var_36  var_37   var_38  var_39  \\\n",
       "0   9.2645  2.0657  12.7753  11.3334  8.1462 -0.0610  3.5331   9.7804  8.7625   \n",
       "1  11.4938 -0.9660  15.3445  10.6361  0.8966  6.7428  2.3421  12.8678 -1.5536   \n",
       "2   9.7320  3.1062  22.3076  11.9593  9.9255  4.0702  4.9934   8.0667  0.8804   \n",
       "3   9.5540  2.3628  10.2124  10.8047 -2.5588  6.0720  3.2613  16.5632  8.8336   \n",
       "4  10.6239 -4.7674  17.5447  11.8668  3.0154  4.2546  6.7601   5.9613  0.3695   \n",
       "\n",
       "    var_40   var_41   var_42   var_43   var_44   var_45   var_46   var_47  \\\n",
       "0 -15.6305  18.8766  11.2864  11.8362  13.3680 -31.9891  12.1776   8.7714   \n",
       "1  10.0309   3.1337  10.5742  11.7664   2.1782 -41.1924  13.5322 -17.3834   \n",
       "2 -19.0841   5.2272   9.5977  12.1801   8.3565  15.1170  10.0921 -20.8504   \n",
       "3  -4.8327   0.9554  12.3754  11.4241   6.6917 -12.9761  13.7343   5.0150   \n",
       "4 -14.4364   5.1392  11.6336  12.0338  18.9670  12.0144  16.2096  -2.1966   \n",
       "\n",
       "    var_48   var_49   var_50   var_51  var_52  var_53  var_54   var_55  \\\n",
       "0  17.2011  16.8508  13.0534  14.4069 -4.8525  7.3213 -0.5259  16.6365   \n",
       "1   6.3806  12.5589  11.6887  25.3930  1.5776  6.8481  8.7348  16.4239   \n",
       "2   8.6758   8.1292  11.8932  10.6869 -0.6434  5.6510  9.3742  25.8831   \n",
       "3  31.3923   5.8555  12.6082   1.4182 -4.1185  6.2536  1.4257  13.5426   \n",
       "4   1.1174  13.4532  12.7925   4.3775 -0.1543  5.6794  0.8210  19.1358   \n",
       "\n",
       "    var_56  var_57  var_58   var_59   var_60   var_61  var_62  var_63  var_64  \\\n",
       "0  19.3036  6.4129 -5.3948   9.3269  11.9314  -3.5750 -0.7706  0.8705  6.9282   \n",
       "1  21.7056  6.9345  1.6678   9.5249   5.3383 -18.7083  1.3382 -1.7401  5.8398   \n",
       "2  19.8701  5.4834 -4.0304   8.5160   8.9776  -5.6619  2.8117  2.5996  9.0986   \n",
       "3  15.4090  6.8761  1.7476  10.0413  15.2857  -4.1378  0.7928  2.5301  8.1458   \n",
       "4  12.6589  6.4394  4.3425   8.7003  12.0586 -10.4753 -0.0337  5.6603  6.2529   \n",
       "\n",
       "   var_65  var_66   var_67  var_68  var_69   var_70  var_71  var_72   var_73  \\\n",
       "0  2.8914  5.9744  17.4851  5.0125 -1.4230  33.3401  0.8018 -4.7906  30.2708   \n",
       "1  3.1051  4.4307  16.0005  5.0306 -7.3365  12.2806  0.6992 -0.7772  21.5123   \n",
       "2  7.1167  4.9466  13.8268  5.0093  4.7782  19.2081  0.4340  0.8459  34.8598   \n",
       "3  2.5738  5.9876  13.0758  5.0087 -9.7824   8.9289  0.4205 -2.5463   2.9428   \n",
       "4  1.5238  4.5356  20.1344  5.0267 -1.8628  39.8219  1.0498 -0.9113  38.5076   \n",
       "\n",
       "    var_74   var_75   var_76   var_77  var_78   var_79   var_80   var_81  \\\n",
       "0  26.8339  21.7205   7.3075  14.0810  3.1192  17.4265   9.4883  16.9060   \n",
       "1   6.7803  18.1896   6.9388  22.1336  6.3755  13.1525   1.9772  14.0406   \n",
       "2  20.7048  16.4953  -9.7077  19.6357  7.6587  15.5744  16.1691  14.3299   \n",
       "3  10.7087  12.2008  12.5465  19.4201  5.5060  14.1586  17.5941  15.4375   \n",
       "4   2.2201   9.5235   8.1522  14.9224  6.1573  15.5221  11.8133  16.7661   \n",
       "\n",
       "    var_82   var_83   var_84   var_85  var_86   var_87   var_88  var_89  \\\n",
       "0  14.5117  10.0276  -0.9706  20.4588  4.7945  20.4160  13.1633  7.9307   \n",
       "1   6.6904   9.9732 -11.5679  20.4525  9.4951   9.6343   8.1252  2.6059   \n",
       "2   1.3360  -0.4412  -0.2830  14.9105 -3.9016  14.6881   7.3220 -5.1443   \n",
       "3 -13.2668  14.0885   4.0357  22.3119  1.8571  16.5210  10.8149  0.3256   \n",
       "4 -14.6524  -0.4469   0.0306  22.5276  6.9774   2.2563   3.5779  1.4268   \n",
       "\n",
       "    var_90  var_91   var_92   var_93   var_94  var_95   var_96   var_97  \\\n",
       "0  -7.6509  7.0834  15.2324  10.1416   5.9156 -0.5775   5.7600  30.3238   \n",
       "1 -17.4201  7.1848  15.3484  10.6522   5.9897  0.3392  10.3516  29.8204   \n",
       "2 -34.3488  7.0194  12.4785   9.6665  13.2595 -0.5624   5.6347   9.5853   \n",
       "3 -21.4797  6.9174   9.9483  10.3696  11.0362  0.1892  19.4321  40.3383   \n",
       "4   9.0680  7.0197  19.7765  10.0499  11.4803  0.2548  16.7029  45.5510   \n",
       "\n",
       "   var_98      ...       var_196_ratio  var_197_ratio  var_198_ratio  \\\n",
       "0  2.1251      ...            1.071594       1.153006       1.027325   \n",
       "1  1.9998      ...            0.859378       0.844767       1.021946   \n",
       "2  1.4515      ...            0.950892       3.092792       1.103173   \n",
       "3  1.4105      ...            1.036400       1.047166       0.835253   \n",
       "4  1.5795      ...            0.876416       1.936247       0.922273   \n",
       "\n",
       "   var_2_ratio  var_22_ratio  var_24_ratio  var_25_ratio  var_26_ratio  \\\n",
       "0     1.075693      0.929048      1.102077      1.084514      0.877153   \n",
       "1     0.962096      1.063508      1.360240      0.948709      1.003233   \n",
       "2     1.094811      1.019312      0.893617      0.943052      0.698982   \n",
       "3     0.888970      0.933459      0.892451      1.148161      1.112962   \n",
       "4     1.072389      1.048108      0.891793      1.052948      6.579071   \n",
       "\n",
       "   var_28_ratio  var_3_ratio  var_32_ratio  var_34_ratio  var_35_ratio  \\\n",
       "0      1.015963     1.104166      1.087910      0.984106      1.113751   \n",
       "1      1.015963     0.902046      0.961710      0.942953      0.902421   \n",
       "2      0.881027     0.969969      1.087236      0.960473      1.134192   \n",
       "3      0.914898     0.949230      1.111000      0.981641      0.869066   \n",
       "4      0.901573     1.035624      0.627534      0.934480      0.882066   \n",
       "\n",
       "   var_36_ratio  var_37_ratio  var_39_ratio  var_4_ratio  var_40_ratio  \\\n",
       "0      0.929617      0.923175      0.846942     1.036313      0.955849   \n",
       "1      1.088766      1.015836      0.912505     0.941976      1.245700   \n",
       "2      0.974184      0.868174      1.058375     0.851617      1.049897   \n",
       "3      1.086045      0.980036      0.846942     0.927008      1.101379   \n",
       "4      0.996479      1.066665      1.042880     0.933657      0.934741   \n",
       "\n",
       "   var_42_ratio  var_43_ratio  var_44_ratio  var_48_ratio  var_49_ratio  \\\n",
       "0      0.997286      0.872239      0.991373      0.972361      0.986368   \n",
       "1      0.863423      0.912171      1.403513      0.992077      0.930214   \n",
       "2      0.000000      1.027443      0.984616      1.013983      1.017188   \n",
       "3      1.018886      1.150795      0.990496      1.146809      0.949642   \n",
       "4      1.031936      0.988943      0.347748      0.926828      0.955335   \n",
       "\n",
       "   var_5_ratio  var_50_ratio  var_51_ratio  var_52_ratio  var_53_ratio  \\\n",
       "0     1.003991      1.000125      0.964512      0.953046      1.148224   \n",
       "1     0.954073      1.278785      0.979607      1.067269      1.172779   \n",
       "2     1.030596      0.996007      0.937493      0.933610      0.816736   \n",
       "3     1.255122      1.072535      0.901461      0.994539      1.118987   \n",
       "4     0.792603      1.040895      1.033227      0.955693      0.816736   \n",
       "\n",
       "   var_55_ratio  var_56_ratio  var_59_ratio  var_6_ratio  var_60_ratio  \\\n",
       "0      1.027804      0.949831      0.900933     0.978014      1.022414   \n",
       "1      1.027804      1.073180      0.822529     1.056568      0.571836   \n",
       "2      3.918750      0.948736      1.223166     0.951430      0.990746   \n",
       "3      1.023795      1.058432      0.886971     0.936735      1.009341   \n",
       "4      0.978541      0.961420      1.225596     1.208582      1.008235   \n",
       "\n",
       "   var_61_ratio  var_62_ratio  var_63_ratio  var_64_ratio  var_66_ratio  \\\n",
       "0      1.083533      1.011738      1.042202      0.967092      0.959436   \n",
       "1      0.916041      0.965992      0.970648      1.032988      0.768972   \n",
       "2      1.069046      0.990688      0.960151      0.976385      0.967642   \n",
       "3      1.083533      0.993678      0.960151      1.010157      0.959436   \n",
       "4      1.004433      1.017591      1.056296      1.049591      0.853070   \n",
       "\n",
       "   var_67_ratio  var_68_ratio  var_69_ratio  var_70_ratio  var_71_ratio  \\\n",
       "0      1.025783      0.880721      1.113630      1.003394      1.055261   \n",
       "1      0.986626      1.142129      0.985562      0.937500      1.003287   \n",
       "2      0.962138      1.032320      2.640762      1.001662      0.926602   \n",
       "3      0.981961      1.203472      0.836357      0.900457      0.918880   \n",
       "4      1.018370      1.048746      1.111200      1.030801      1.174515   \n",
       "\n",
       "   var_72_ratio  var_73_ratio  var_74_ratio  var_75_ratio  var_76_ratio  \\\n",
       "0      1.002495      1.027508      1.014519      0.927862      0.968576   \n",
       "1      1.033226      1.014497      0.874244      0.995326      0.965819   \n",
       "2      0.985500      1.024586      0.954614      1.069513      2.809890   \n",
       "3      0.975406      0.000000      0.889981      0.947440      1.011655   \n",
       "4      1.033226      1.024075      0.812347      0.806520      0.990602   \n",
       "\n",
       "   var_78_ratio  var_79_ratio  var_80_ratio  var_81_ratio  var_82_ratio  \\\n",
       "0      1.071162      2.951772      0.988266      1.093841      1.541832   \n",
       "1      1.036067      1.027893      0.898107      0.870138      1.161277   \n",
       "2      1.278505      1.065451      1.230617      0.893605      1.072888   \n",
       "3      0.871881      0.944015      1.089865      0.987152      0.899392   \n",
       "4      0.986935      1.065451      1.113453      1.086079      1.017064   \n",
       "\n",
       "   var_83_ratio  var_84_ratio  var_85_ratio  var_86_ratio  var_88_ratio  \\\n",
       "0      1.025807      1.008037      0.926751      0.924111      1.150322   \n",
       "1      1.009023      0.886786      0.926751      1.082121      1.231490   \n",
       "2      1.033928      1.036610      1.135605      0.973719      1.161012   \n",
       "3      1.165655      1.110395      0.938730      0.947992      0.813170   \n",
       "4      1.033928      1.071710      0.954951      0.993734      2.300122   \n",
       "\n",
       "   var_89_ratio  var_9_ratio  var_90_ratio  var_91_ratio  var_92_ratio  \\\n",
       "0      1.141009     1.026816      1.036020      1.017468      0.957805   \n",
       "1      0.893753     1.642776      0.983993      1.000640      0.958733   \n",
       "2      0.000000     1.086410      0.887029      1.010778      1.059650   \n",
       "3      0.904958     0.973884      0.967430      0.946355      1.008301   \n",
       "4      0.876417     0.861295      1.706445      1.010778      1.058171   \n",
       "\n",
       "   var_93_ratio  var_94_ratio  var_95_ratio  var_96_ratio  var_97_ratio  \\\n",
       "0      0.899621      0.592629      0.955423      0.884545      1.003864   \n",
       "1      0.991959      0.644963      1.030191      0.910490      1.003864   \n",
       "2      1.348678      1.081930      0.955423      0.884545      0.962684   \n",
       "3      0.931385      1.015048      0.997093      1.023441      0.995909   \n",
       "4      0.897156      0.959945      0.997693      0.997531      1.069630   \n",
       "\n",
       "   var_99_ratio  var_104_ratio  var_107_ratio  var_112_ratio  var_121_ratio  \\\n",
       "0      0.981864       1.018372       0.986088       1.166043       1.004279   \n",
       "1      0.949477       1.017850       0.634520       1.049403       0.968906   \n",
       "2      0.997798       0.983745       1.120921       0.873900       0.868519   \n",
       "3      0.981609       1.013304       0.963549       1.186202       1.178879   \n",
       "4      1.022511       1.053644       0.993833       0.867665       1.015097   \n",
       "\n",
       "   var_14_ratio  var_140_ratio  var_142_ratio  var_149_ratio  var_156_ratio  \\\n",
       "0      1.004569       2.899864       1.173750       0.969311       0.885866   \n",
       "1      1.009463       1.011640       0.735335       0.000000       1.141756   \n",
       "2      1.076684       2.187513       0.725777       0.977746       1.068856   \n",
       "3      0.998691       0.993216       0.885354       0.000000       0.983195   \n",
       "4      0.928777       1.009986       1.377834       1.172329       1.044696   \n",
       "\n",
       "   var_160_ratio  var_172_ratio  var_177_ratio  var_178_ratio  var_186_ratio  \\\n",
       "0       1.057211       1.045474       1.312693       0.960050       1.022345   \n",
       "1       0.957236       1.075419       0.656129       0.940605       0.953552   \n",
       "2       0.841795       1.005458       1.042237       0.915866       1.064954   \n",
       "3       1.134118       0.920478       0.815710       1.036581       0.705848   \n",
       "4       1.087812       0.992274       1.307596       0.936262       1.033690   \n",
       "\n",
       "   var_192_ratio  var_199_ratio  var_20_ratio  var_21_ratio  var_23_ratio  \\\n",
       "0       0.000000       0.876353      0.318066      0.972321      1.036051   \n",
       "1       0.904601       2.066700      1.045679      0.998414      0.838075   \n",
       "2       1.003664       3.109997      1.031274      0.977352      0.997614   \n",
       "3       1.000832       0.923711      1.009285      0.977352      1.221038   \n",
       "4       0.943855       0.876353      0.000000      1.028467      0.956553   \n",
       "\n",
       "   var_31_ratio  var_33_ratio  var_45_ratio  var_57_ratio  var_65_ratio  \\\n",
       "0      1.001298      1.200054      0.945387      0.996193      1.002181   \n",
       "1      1.003727      0.991604      0.976678      1.001069      1.007946   \n",
       "2      1.003424      1.400196      1.036542      0.898564      1.006031   \n",
       "3      1.013438      1.003302      0.945524      1.005237      0.997823   \n",
       "4      0.997273      0.833296      1.057838      1.003821      1.008596   \n",
       "\n",
       "   var_77_ratio  var_8_ratio  var_87_ratio  var_120_ratio  var_46_ratio  \\\n",
       "0      0.897869     1.086287      1.085689       1.135204      1.033577   \n",
       "1      0.956957     0.972639      1.041837       0.940335      1.050002   \n",
       "2      1.006857     1.052565      0.993483       1.039746      0.934629   \n",
       "3      1.010598     1.136648      0.965914       0.968145      1.036233   \n",
       "4      0.916172     1.135200      0.960790       1.100778      1.046678   \n",
       "\n",
       "   var_54_ratio  var_58_ratio  \n",
       "0      0.921647      1.044576  \n",
       "1      1.098613      0.967202  \n",
       "2      1.098764      1.234375  \n",
       "3      0.972784      0.967202  \n",
       "4      0.966339      1.023166  \n",
       "\n",
       "[5 rows x 381 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "Distribution of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "6f7cd5bdd625e69c75e10f586a826da80c814cdf"
   },
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "predictors = train_df.columns.values.tolist()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "cc8cef4f66aa77c4183cc4f99acd6082b6f036bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    179902\n",
       "1     20098\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad51d7daad193e0dab467f82dfad4c7ce7876d56"
   },
   "source": [
    "The problem is unbalanced! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "496907f42901e10bb883858252b2783e30ff2e43"
   },
   "source": [
    "In this kernel I will be using **50% Stratified rows** as holdout rows for the validation-set to get optimal parameters. Later I will use 5 fold cross validation in the final model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "7ec87a3460c6358b9a134afea5bac561f7a84226"
   },
   "outputs": [],
   "source": [
    "bayesian_tr_index, bayesian_val_index  = list(StratifiedKFold(n_splits=2, shuffle=True, random_state=1).split(train_df, train_df.target.values))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cbcebd0aacaeb637a1e119971303c9fcd60f9ea5"
   },
   "source": [
    "These `bayesian_tr_index` and `bayesian_val_index` indexes will be used for the bayesian optimization as training and validation index of training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f18184730f7261c3ddc2253ee025f9910ffedb6"
   },
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## 2. Black box function to be optimized (LightGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a5be20e9809494709c5da85861775c8720816ba"
   },
   "source": [
    "As data is loaded, let's create the black box function for LightGBM to find parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "196288ebb7caf614e230a0449b40354266efbc45"
   },
   "outputs": [],
   "source": [
    "def LGB_bayesian(\n",
    "    num_leaves,  # int\n",
    "    min_data_in_leaf,  # int\n",
    "    learning_rate,\n",
    "    min_sum_hessian_in_leaf,    # int  \n",
    "    feature_fraction,\n",
    "    lambda_l1,\n",
    "    lambda_l2,\n",
    "    min_gain_to_split,\n",
    "    max_depth):\n",
    "    \n",
    "    # LightGBM expects next three parameters need to be integer. So we make them integer\n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "\n",
    "    param = {\n",
    "        'num_leaves': num_leaves,\n",
    "        'max_bin': 63,\n",
    "        'min_data_in_leaf': min_data_in_leaf,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n",
    "        'bagging_fraction': 1.0,\n",
    "        'bagging_freq': 5,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'min_gain_to_split': min_gain_to_split,\n",
    "        'max_depth': max_depth,\n",
    "        'save_binary': True, \n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'drop_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False,   \n",
    "\n",
    "    }    \n",
    "    \n",
    "    \n",
    "    xg_train = lgb.Dataset(train_df.iloc[bayesian_tr_index][predictors].values,\n",
    "                           label=train_df.iloc[bayesian_tr_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )\n",
    "    xg_valid = lgb.Dataset(train_df.iloc[bayesian_val_index][predictors].values,\n",
    "                           label=train_df.iloc[bayesian_val_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )   \n",
    "\n",
    "    num_round = 5000\n",
    "    clf = lgb.train(param, xg_train, num_round, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n",
    "    \n",
    "    predictions = clf.predict(train_df.iloc[bayesian_val_index][predictors].values, num_iteration=clf.best_iteration)   \n",
    "    \n",
    "    score = metrics.roc_auc_score(train_df.iloc[bayesian_val_index][target].values, predictions)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40b3959e5f0672b6e030408cd197da137e1b66ee"
   },
   "source": [
    "The above `LGB_bayesian` function will act as black box function for Bayesian optimization. I already defined the the trainng and validation dataset for LightGBM inside the `LGB_bayesian` function. \n",
    "\n",
    "The `LGB_bayesian` function takes values for `num_leaves`, `min_data_in_leaf`, `learning_rate`, `min_sum_hessian_in_leaf`, `feature_fraction`, `lambda_l1`, `lambda_l2`, `min_gain_to_split`, `max_depth` from Bayesian optimization framework. Keep in mind that `num_leaves`, `min_data_in_leaf`, and `max_depth` should be integer for LightGBM. But Bayesian Optimization sends continous vales to function. So I force them to be integer. I am only going to find optimal parameter values of them. The reader may increase or decrease number of parameters to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fb52d3d130c2477a6ab71b2d6797c787dce9f21"
   },
   "source": [
    "Now I need to give bounds for these parameters, so that Bayesian optimization only search inside the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "044ed09d293f7a712af6fe6c00e935df19ca1cf4"
   },
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (5, 20), \n",
    "    'min_data_in_leaf': (5, 20),  \n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'min_sum_hessian_in_leaf': (0.00001, 0.01),    \n",
    "    'feature_fraction': (0.05, 0.5),\n",
    "    'lambda_l1': (0, 5.0), \n",
    "    'lambda_l2': (0, 5.0), \n",
    "    'min_gain_to_split': (0, 1.0),\n",
    "    'max_depth':(3,15),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a2a217d1bcc8b27e67b9cc7f4b7bbb237b8ee72"
   },
   "source": [
    "Let's put all of them in BayesianOptimization object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "c0fc26566d5b5bf64a9f507cd06bf0ab85b584e3"
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "af6565a9d43d2b5a153a3f2009afca7618a699c1"
   },
   "outputs": [],
   "source": [
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95f597ee9781e5621c98fe89d69a30601902c716"
   },
   "source": [
    "Now, let's the the key space (parameters) we are going to optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "281f53a7d9bca23329e965986939443bc63a927c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num_leaves', 'min_data_in_leaf', 'learning_rate', 'min_sum_hessian_in_leaf', 'feature_fraction', 'lambda_l1', 'lambda_l2', 'min_gain_to_split', 'max_depth']\n"
     ]
    }
   ],
   "source": [
    "print(LGB_BO.space.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2fbbcbe963f659df1884511af1a0009f28ef6d5"
   },
   "source": [
    "I have created the BayesianOptimization object (`LGB_BO`), it will not work until I call maximize. Before calling it, I want to explain two parameters of BayesianOptimization object (`LGB_BO`) which we can pass to maximize:\n",
    "- `init_points`: How many initial random runs of **random** exploration we want to perform. In our case `LGB_bayesian` will be called `n_iter` times.\n",
    "- `n_iter`: How many runs of bayesian optimization we want to perform after number of `init_points` runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e06903e320665c4f77e9b029681cf6f651dfc9b4"
   },
   "source": [
    "Now, it's time to call the function from Bayesian optimization framework to maximize. I allow `LGB_BO` object to run for 5 `init_points` (exploration) and 5 `n_iter` (exploitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "6c12e5f092c2d0f690fd720e40aec69268c9e70f"
   },
   "outputs": [],
   "source": [
    "init_points = 5\n",
    "n_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "f5f6d3f1a7752d818330b92980344956bf934e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   feature_fraction |   lambda_l1 |   lambda_l2 |   learning_rate |   max_depth |   min_data_in_leaf |   min_gain_to_split |   min_sum_hessian_in_leaf |   num_leaves | \n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.832919\n",
      "[500]\tvalid_0's auc: 0.860563\n",
      "[750]\tvalid_0's auc: 0.87262\n",
      "[1000]\tvalid_0's auc: 0.879457\n",
      "[1250]\tvalid_0's auc: 0.883684\n",
      "[1500]\tvalid_0's auc: 0.886434\n",
      "[1750]\tvalid_0's auc: 0.888416\n",
      "[2000]\tvalid_0's auc: 0.889621\n",
      "[2250]\tvalid_0's auc: 0.890417\n",
      "[2500]\tvalid_0's auc: 0.890861\n",
      "[2750]\tvalid_0's auc: 0.891144\n",
      "Early stopping, best iteration is:\n",
      "[2750]\tvalid_0's auc: 0.891144\n",
      "    1 | 03m32s | \u001b[35m   0.89114\u001b[0m | \u001b[32m            0.4771\u001b[0m | \u001b[32m     0.3254\u001b[0m | \u001b[32m     4.0642\u001b[0m | \u001b[32m         0.0202\u001b[0m | \u001b[32m     6.3240\u001b[0m | \u001b[32m           11.8017\u001b[0m | \u001b[32m             0.9556\u001b[0m | \u001b[32m                   0.0068\u001b[0m | \u001b[32m     16.6655\u001b[0m | \n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.876312\n",
      "[500]\tvalid_0's auc: 0.888221\n",
      "[750]\tvalid_0's auc: 0.891001\n",
      "Early stopping, best iteration is:\n",
      "[784]\tvalid_0's auc: 0.891175\n",
      "    2 | 00m32s | \u001b[35m   0.89117\u001b[0m | \u001b[32m            0.1481\u001b[0m | \u001b[32m     3.1491\u001b[0m | \u001b[32m     0.3786\u001b[0m | \u001b[32m         0.0966\u001b[0m | \u001b[32m    11.3453\u001b[0m | \u001b[32m           14.1356\u001b[0m | \u001b[32m             0.0000\u001b[0m | \u001b[32m                   0.0026\u001b[0m | \u001b[32m      8.5631\u001b[0m | \n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.853481\n",
      "[500]\tvalid_0's auc: 0.874818\n",
      "[750]\tvalid_0's auc: 0.883642\n",
      "[1000]\tvalid_0's auc: 0.887949\n",
      "[1250]\tvalid_0's auc: 0.890457\n",
      "[1500]\tvalid_0's auc: 0.891945\n",
      "[1750]\tvalid_0's auc: 0.892629\n",
      "[2000]\tvalid_0's auc: 0.892995\n",
      "Early stopping, best iteration is:\n",
      "[2108]\tvalid_0's auc: 0.893065\n",
      "    3 | 03m06s | \u001b[35m   0.89306\u001b[0m | \u001b[32m            0.1937\u001b[0m | \u001b[32m     4.3691\u001b[0m | \u001b[32m     3.2823\u001b[0m | \u001b[32m         0.0270\u001b[0m | \u001b[32m    14.0226\u001b[0m | \u001b[32m           16.6329\u001b[0m | \u001b[32m             0.2470\u001b[0m | \u001b[32m                   0.0035\u001b[0m | \u001b[32m     17.3642\u001b[0m | \n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.872725\n",
      "Early stopping, best iteration is:\n",
      "[282]\tvalid_0's auc: 0.872977\n",
      "    4 | 00m31s |    0.87298 |             0.4630 |      0.0436 |      2.5463 |          0.2585 |      5.9337 |            14.6242 |              0.7122 |                    0.0001 |      19.4862 | \n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.884106\n",
      "[500]\tvalid_0's auc: 0.888419\n",
      "Early stopping, best iteration is:\n",
      "[565]\tvalid_0's auc: 0.888725\n",
      "    5 | 00m38s |    0.88873 |             0.0644 |      3.7329 |      2.3994 |          0.1181 |      8.4970 |            15.8303 |              0.3246 |                    0.0036 |      19.5890 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   feature_fraction |   lambda_l1 |   lambda_l2 |   learning_rate |   max_depth |   min_data_in_leaf |   min_gain_to_split |   min_sum_hessian_in_leaf |   num_leaves | \n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid_0's auc: 0.874962\n",
      "    6 | 01m44s |    0.87496 |             0.5000 |      5.0000 |      0.0000 |          0.3000 |     15.0000 |             5.0000 |              1.0000 |                    0.0100 |      20.0000 | \n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.882756\n",
      "Early stopping, best iteration is:\n",
      "[449]\tvalid_0's auc: 0.888262\n",
      "    7 | 00m24s |    0.88826 |             0.1583 |      4.9722 |      4.7605 |          0.2683 |      3.1986 |             5.5652 |              0.3307 |                    0.0031 |       5.0324 | \n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.853582\n",
      "[500]\tvalid_0's auc: 0.875216\n",
      "[750]\tvalid_0's auc: 0.883543\n",
      "[1000]\tvalid_0's auc: 0.88826\n",
      "[1250]\tvalid_0's auc: 0.890706\n",
      "[1500]\tvalid_0's auc: 0.892063\n",
      "[1750]\tvalid_0's auc: 0.892954\n",
      "Early stopping, best iteration is:\n",
      "[1743]\tvalid_0's auc: 0.892972\n",
      "    8 | 00m43s |    0.89297 |             0.0662 |      4.6180 |      4.4040 |          0.0685 |      3.2973 |            19.4061 |              0.7161 |                    0.0067 |       5.6148 | \n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.857105\n",
      "[500]\tvalid_0's auc: 0.876341\n",
      "[750]\tvalid_0's auc: 0.884948\n",
      "[1000]\tvalid_0's auc: 0.889395\n",
      "[1250]\tvalid_0's auc: 0.891788\n",
      "[1500]\tvalid_0's auc: 0.892852\n",
      "Early stopping, best iteration is:\n",
      "[1673]\tvalid_0's auc: 0.893399\n",
      "    9 | 00m35s | \u001b[35m   0.89340\u001b[0m | \u001b[32m            0.0535\u001b[0m | \u001b[32m     4.0539\u001b[0m | \u001b[32m     4.8822\u001b[0m | \u001b[32m         0.0674\u001b[0m | \u001b[32m    14.4564\u001b[0m | \u001b[32m            6.0508\u001b[0m | \u001b[32m             0.1449\u001b[0m | \u001b[32m                   0.0009\u001b[0m | \u001b[32m      5.9100\u001b[0m | \n",
      "Train until valid scores didn't improve in 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.832879\n",
      "[500]\tvalid_0's auc: 0.859758\n",
      "[750]\tvalid_0's auc: 0.8718\n",
      "[1000]\tvalid_0's auc: 0.878395\n",
      "[1250]\tvalid_0's auc: 0.88251\n",
      "[1500]\tvalid_0's auc: 0.885103\n",
      "[1750]\tvalid_0's auc: 0.887017\n",
      "[2000]\tvalid_0's auc: 0.888233\n",
      "[2250]\tvalid_0's auc: 0.88908\n",
      "[2500]\tvalid_0's auc: 0.889727\n",
      "[2750]\tvalid_0's auc: 0.890103\n",
      "[3000]\tvalid_0's auc: 0.890498\n",
      "Early stopping, best iteration is:\n",
      "[3103]\tvalid_0's auc: 0.890562\n",
      "   10 | 02m37s |    0.89056 |             0.2974 |      4.3986 |      4.8138 |          0.0220 |      5.6912 |             5.1280 |              0.1298 |                    0.0070 |      19.5708 | \n"
     ]
    }
   ],
   "source": [
    "print('-' * 130)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bdea40277b15b11b5b92238e519cf5fc2576f64f"
   },
   "source": [
    "As the optimization is done, let's see what is the maximum value we have got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "d0d5a3fe41a7d9f1625cdfb5eceb113f6cabcf93"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BayesianOptimization' object has no attribute 'max'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-bac10f5a9b96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLGB_BO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'BayesianOptimization' object has no attribute 'max'"
     ]
    }
   ],
   "source": [
    "LGB_BO.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "pbounds = {'x': (2, 4), 'y': (-3, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box_function(x, y):\n",
    "    \"\"\"Function with unknown internals we wish to maximize.\n",
    "\n",
    "    This is just serving as an example, for all intents and\n",
    "    purposes think of the internals of this function, i.e.: the process\n",
    "    which generates its output values, as unknown.\n",
    "    \"\"\"\n",
    "    return -x ** 2 - (y - 1) ** 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=black_box_function,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad0a200a70e00955a924ca5f3fc622b492eaf3e3"
   },
   "source": [
    "The validation AUC for parameters is 0.89 ! Let's see parameters is responsible for this score :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |         x |         y | \n",
      "    1 | 00m00s | \u001b[35m -23.02632\u001b[0m | \u001b[32m   2.8340\u001b[0m | \u001b[32m  -2.9993\u001b[0m | \n",
      "    2 | 00m00s | \u001b[35m -15.61668\u001b[0m | \u001b[32m   3.4406\u001b[0m | \u001b[32m  -1.1860\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |         x |         y | \n",
      "    3 | 00m00s |  -19.00000 |    4.0000 |    3.0000 | \n",
      "    4 | 00m01s | \u001b[35m  -3.07171\u001b[0m | \u001b[32m   2.0000\u001b[0m | \u001b[32m   0.7322\u001b[0m | \n",
      "    5 | 00m01s | \u001b[35m  -3.02674\u001b[0m | \u001b[32m   2.0000\u001b[0m | \u001b[32m   1.1635\u001b[0m | \n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BayesianOptimization' object has no attribute 'max'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-ea8ff8c71a3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'BayesianOptimization' object has no attribute 'max'"
     ]
    }
   ],
   "source": [
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BayesianOptimization' object has no attribute 'probe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-6a3175959b1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m optimizer.probe(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"y\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BayesianOptimization' object has no attribute 'probe'"
     ]
    }
   ],
   "source": [
    "optimizer.probe(\n",
    "    params={\"x\": 0.5, \"y\": 0.7},\n",
    "    lazy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: \n",
      "\tmax\n",
      "Iteration 1: \n",
      "\tall\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ced952bfd9397a68e3a2d394a4f7e075b40a9aff"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BayesianOptimization' object has no attribute 'max'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c6e77f382389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLGB_BO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'BayesianOptimization' object has no attribute 'max'"
     ]
    }
   ],
   "source": [
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f6bc1d0859b643bdbbeba1e0e3c8b336860c023"
   },
   "source": [
    "Now we can use these parameters to our final model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06096bd955639e891320f5b31408fad1785f5949"
   },
   "source": [
    "Wait, I want to show one more cool option from BayesianOptimization library. You can probe the `LGB_bayesian` function, if you have an idea of the optimal parameters or it you get **parameters from other kernel** like mine [mine](https://www.kaggle.com/fayzur/customer-transaction-prediction). I will copy and paste parameters from my other kernel here. You can probe as folowing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "a0275beb85200e0c8cce95b477dd756e8d1ef2ef"
   },
   "outputs": [],
   "source": [
    "# parameters from version 2 of\n",
    "#https://www.kaggle.com/fayzur/customer-transaction-prediction?scriptVersionId=10522231\n",
    "\n",
    "LGB_BO.probe(\n",
    "    params={'feature_fraction': 0.1403, \n",
    "            'lambda_l1': 4.218, \n",
    "            'lambda_l2': 1.734, \n",
    "            'learning_rate': 0.07, \n",
    "            'max_depth': 14, \n",
    "            'min_data_in_leaf': 17, \n",
    "            'min_gain_to_split': 0.1501, \n",
    "            'min_sum_hessian_in_leaf': 0.000446, \n",
    "            'num_leaves': 6},\n",
    "    lazy=True, # \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2893bc7de2bf69fd02ef26e90fc354f71b83e10"
   },
   "source": [
    "OK, by default these will be explored lazily (lazy=True), meaning these points will be evaluated only the next time you call maximize. Let's do a maximize call of `LGB_BO` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "8e3feb820520bb03000225416e338610e2d90b1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.86243\n",
      "[500]\tvalid_0's auc: 0.880738\n",
      "[750]\tvalid_0's auc: 0.887841\n",
      "[1000]\tvalid_0's auc: 0.890905\n",
      "[1250]\tvalid_0's auc: 0.892214\n",
      "[1500]\tvalid_0's auc: 0.892655\n",
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's auc: 0.892672\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.8927  \u001b[0m | \u001b[95m 0.1403  \u001b[0m | \u001b[95m 4.218   \u001b[0m | \u001b[95m 1.734   \u001b[0m | \u001b[95m 0.07    \u001b[0m | \u001b[95m 14.0    \u001b[0m | \u001b[95m 17.0    \u001b[0m | \u001b[95m 0.1501  \u001b[0m | \u001b[95m 0.000446\u001b[0m | \u001b[95m 6.0     \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "LGB_BO.maximize(init_points=0, n_iter=0) # remember no init_points or n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9bd70047d5313694681d73ed049339d6b00261c0"
   },
   "source": [
    "Finally, the list of all parameters probed and their corresponding target values is available via the property LGB_BO.res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "416bfb9650849e75106ee63b40dfd7519aa2ee29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: \n",
      "\t{'target': 0.8785456504868869, 'params': {'feature_fraction': 0.3999660847582191, 'lambda_l1': 1.1877061001745615, 'lambda_l2': 4.1213926633068425, 'learning_rate': 0.2900672674324699, 'max_depth': 14.67121336685872, 'min_data_in_leaf': 11.801738711259683, 'min_gain_to_split': 0.6090424627612779, 'min_sum_hessian_in_leaf': 0.007757509880902418, 'num_leaves': 14.624200171386038}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.8915776403640969, 'params': {'feature_fraction': 0.3749082032826262, 'lambda_l1': 0.17518262050718658, 'lambda_l2': 1.492247354445897, 'learning_rate': 0.026968622645801674, 'max_depth': 13.284731311046386, 'min_data_in_leaf': 10.592810418122113, 'min_gain_to_split': 0.679847951578097, 'min_sum_hessian_in_leaf': 0.002570236693773035, 'num_leaves': 10.21371822728738}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.8924701144136037, 'params': {'feature_fraction': 0.05423574653643624, 'lambda_l1': 1.7916689135248487, 'lambda_l2': 4.745470908391052, 'learning_rate': 0.07319071264818977, 'max_depth': 6.8326963965643746, 'min_data_in_leaf': 18.76658579000881, 'min_gain_to_split': 0.03190366643989473, 'min_sum_hessian_in_leaf': 0.0006601945250547198, 'num_leaves': 14.447434986617345}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.8843155956741141, 'params': {'feature_fraction': 0.4432160494757924, 'lambda_l1': 0.04357866151892431, 'lambda_l2': 3.7328861849696877, 'learning_rate': 0.24572393959076078, 'max_depth': 3.9086093540672007, 'min_data_in_leaf': 14.846830018954368, 'min_gain_to_split': 0.5092622000835182, 'min_sum_hessian_in_leaf': 0.004804035081048867, 'num_leaves': 19.333612173965996}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.8918922376241952, 'params': {'feature_fraction': 0.0500054151062683, 'lambda_l1': 1.2348935049595817, 'lambda_l2': 3.5611633895579176, 'learning_rate': 0.1041287944381468, 'max_depth': 6.323956276605713, 'min_data_in_leaf': 15.431681788302118, 'min_gain_to_split': 0.9185517481459488, 'min_sum_hessian_in_leaf': 0.0024523122649579236, 'num_leaves': 11.871287259151455}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.8877741089318031, 'params': {'feature_fraction': 0.28880235499863005, 'lambda_l1': 0.0676352739721825, 'lambda_l2': 0.06367783660314708, 'learning_rate': 0.23658833958998987, 'max_depth': 8.82154329429286, 'min_data_in_leaf': 19.911222977094404, 'min_gain_to_split': 0.7962518122529608, 'min_sum_hessian_in_leaf': 0.00982563254531773, 'num_leaves': 5.699237677594455}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.8838845198764629, 'params': {'feature_fraction': 0.5, 'lambda_l1': 5.0, 'lambda_l2': 1.2648294958898453e-09, 'learning_rate': 0.010000000280869852, 'max_depth': 3.0, 'min_data_in_leaf': 5.000000002619426, 'min_gain_to_split': 0.0, 'min_sum_hessian_in_leaf': 1.0000022537751995e-05, 'num_leaves': 20.0}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.8914029939138292, 'params': {'feature_fraction': 0.24571681728644235, 'lambda_l1': 0.46805485709572603, 'lambda_l2': 4.634252859543099, 'learning_rate': 0.10893673688261334, 'max_depth': 5.74869199423548, 'min_data_in_leaf': 5.199199561225826, 'min_gain_to_split': 0.03917651107600029, 'min_sum_hessian_in_leaf': 0.007056042775055096, 'num_leaves': 5.49237053333988}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.885671663972105, 'params': {'feature_fraction': 0.3120275776133379, 'lambda_l1': 5.0, 'lambda_l2': 9.084821479118918e-09, 'learning_rate': 0.01, 'max_depth': 14.999999991290764, 'min_data_in_leaf': 5.0, 'min_gain_to_split': 1.0, 'min_sum_hessian_in_leaf': 0.009999999929977302, 'num_leaves': 5.0}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.8896394504207458, 'params': {'feature_fraction': 0.46632352517727926, 'lambda_l1': 4.554786486122389, 'lambda_l2': 0.07813817250912569, 'learning_rate': 0.1009416028187477, 'max_depth': 3.3923822872270604, 'min_data_in_leaf': 10.028922050131339, 'min_gain_to_split': 0.01464358752550321, 'min_sum_hessian_in_leaf': 0.008685058946939706, 'num_leaves': 7.056018802118709}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.8926723153666576, 'params': {'feature_fraction': 0.1403, 'lambda_l1': 4.218, 'lambda_l2': 1.734, 'learning_rate': 0.07, 'max_depth': 14.0, 'min_data_in_leaf': 17.0, 'min_gain_to_split': 0.1501, 'min_sum_hessian_in_leaf': 0.000446, 'num_leaves': 6.0}}\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(LGB_BO.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32a1bd2e328c121915b77177e72f281e6b64ca4c"
   },
   "source": [
    "We have got a better validation score in the probe! As previously I ran `LGB_BO` only for 10 runs. In practice I increase it to arround 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "5e881cbd9dc198f4cc91bb65823542ec473c6b39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8926723153666576"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "1a132698b07c7652d62c329f5d6508211540834f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_fraction': 0.1403,\n",
       " 'lambda_l1': 4.218,\n",
       " 'lambda_l2': 1.734,\n",
       " 'learning_rate': 0.07,\n",
       " 'max_depth': 14.0,\n",
       " 'min_data_in_leaf': 17.0,\n",
       " 'min_gain_to_split': 0.1501,\n",
       " 'min_sum_hessian_in_leaf': 0.000446,\n",
       " 'num_leaves': 6.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e17fc50406f38145fffae3c4aa585f5520433cab"
   },
   "source": [
    "Let's build a model together use therse parameters ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b91db8b5c98da4f014f1863ba7de18e241f517c6"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## 3. Training LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "56d006d366fc5c2686249887b5d1f302d4a708f5"
   },
   "outputs": [],
   "source": [
    "param_lgb = {\n",
    "        'num_leaves': int(LGB_BO.max['params']['num_leaves']), # remember to int here\n",
    "        'max_bin': 63,\n",
    "        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), # remember to int here\n",
    "        'learning_rate': LGB_BO.max['params']['learning_rate'],\n",
    "        'min_sum_hessian_in_leaf': LGB_BO.max['params']['min_sum_hessian_in_leaf'],\n",
    "        'bagging_fraction': 1.0, \n",
    "        'bagging_freq': 5, \n",
    "        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n",
    "        'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n",
    "        'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n",
    "        'min_gain_to_split': LGB_BO.max['params']['min_gain_to_split'],\n",
    "        'max_depth': int(LGB_BO.max['params']['max_depth']), # remember to int here\n",
    "        'save_binary': True,\n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'drop_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5a87047368fe7504c895aab4e9deb1b43746d7d"
   },
   "source": [
    "As you see, I assined `LGB_BO`'s optimal parameters to the `param_lgb` dictionary and they will be used to train a model with 5 fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ab8ef627687c7602b81a9ac83f90ff3a2094d0c"
   },
   "source": [
    "Number of Kfolds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "9f306e20ae748715da17a2f15702ea1aa4d81497"
   },
   "outputs": [],
   "source": [
    "nfold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "f2f01a6006dcb34ffb53ca33a055e592ee9e75e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "0e9f0fd37207aeaa33f3d758ed22a32b462fc93d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "0e9f0fd37207aeaa33f3d758ed22a32b462fc93d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold 1\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.865007\n",
      "[500]\tvalid_0's auc: 0.883335\n",
      "[750]\tvalid_0's auc: 0.890893\n",
      "[1000]\tvalid_0's auc: 0.894616\n",
      "[1250]\tvalid_0's auc: 0.896093\n",
      "[1500]\tvalid_0's auc: 0.896619\n",
      "Early stopping, best iteration is:\n",
      "[1603]\tvalid_0's auc: 0.89686\n",
      "\n",
      "fold 2\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.860373\n",
      "[500]\tvalid_0's auc: 0.879811\n",
      "[750]\tvalid_0's auc: 0.88715\n",
      "[1000]\tvalid_0's auc: 0.891324\n",
      "[1250]\tvalid_0's auc: 0.893482\n",
      "[1500]\tvalid_0's auc: 0.894638\n",
      "Early stopping, best iteration is:\n",
      "[1545]\tvalid_0's auc: 0.894804\n",
      "\n",
      "fold 3\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.86603\n",
      "[500]\tvalid_0's auc: 0.884838\n",
      "[750]\tvalid_0's auc: 0.892292\n",
      "[1000]\tvalid_0's auc: 0.895874\n",
      "[1250]\tvalid_0's auc: 0.897865\n",
      "Early stopping, best iteration is:\n",
      "[1310]\tvalid_0's auc: 0.898187\n",
      "\n",
      "fold 4\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.858726\n",
      "[500]\tvalid_0's auc: 0.879387\n",
      "[750]\tvalid_0's auc: 0.886873\n",
      "[1000]\tvalid_0's auc: 0.890344\n",
      "[1250]\tvalid_0's auc: 0.891781\n",
      "[1500]\tvalid_0's auc: 0.892681\n",
      "Early stopping, best iteration is:\n",
      "[1662]\tvalid_0's auc: 0.893023\n",
      "\n",
      "fold 5\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[250]\tvalid_0's auc: 0.861535\n",
      "[500]\tvalid_0's auc: 0.880695\n",
      "[750]\tvalid_0's auc: 0.888037\n",
      "[1000]\tvalid_0's auc: 0.891692\n",
      "[1250]\tvalid_0's auc: 0.89359\n",
      "[1500]\tvalid_0's auc: 0.89438\n",
      "Early stopping, best iteration is:\n",
      "[1600]\tvalid_0's auc: 0.894584\n",
      "\n",
      "\n",
      "CV AUC: 0.90\n"
     ]
    }
   ],
   "source": [
    "oof = np.zeros(len(train_df))\n",
    "predictions = np.zeros((len(test_df),nfold))\n",
    "\n",
    "i = 1\n",
    "for train_index, valid_index in skf.split(train_df, train_df.target.values):\n",
    "    print(\"\\nfold {}\".format(i))\n",
    "    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n",
    "                           label=train_df.iloc[train_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )\n",
    "    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n",
    "                           label=train_df.iloc[valid_index][target].values,\n",
    "                           feature_name=predictors,\n",
    "                           free_raw_data = False\n",
    "                           )   \n",
    "\n",
    "    \n",
    "    clf = lgb.train(param_lgb, xg_train, 5000, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n",
    "    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n",
    "    \n",
    "    predictions[:,i-1] += clf.predict(test_df[predictors], num_iteration=clf.best_iteration)\n",
    "    i = i + 1\n",
    "\n",
    "print(\"\\n\\nCV AUC: {:<0.2f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6c8fde3d71858028688bd9a62bf1a4430b6bbb1"
   },
   "source": [
    "So we got 0.90 AUC in 5 fold cross validation. And 5 fold prediction look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "a889d2c1f63dbe989b3f0b0373ce9e1c80e3ee10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41628346, 0.39960379, 0.38409074, 0.35182696, 0.40993432],\n",
       "       [0.63407781, 0.6236449 , 0.61534627, 0.66678195, 0.66385472],\n",
       "       [0.65491558, 0.7424437 , 0.63334601, 0.60578336, 0.70685663],\n",
       "       ...,\n",
       "       [0.05168246, 0.0319463 , 0.04241488, 0.03894831, 0.03609798],\n",
       "       [0.41015456, 0.48430607, 0.53886698, 0.48510382, 0.49223725],\n",
       "       [0.40877008, 0.39991497, 0.44468547, 0.48166165, 0.41461139]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01339e95305a72802fd23a2c740cb7cc988d1c27"
   },
   "source": [
    "If you are still reading, bare with me. I will not take much of your time. :D We are almost done. Let's do a rank averaging on 5 fold predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "116f2c0c3c43095d456a88d50425de0a3e7fdd11"
   },
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "## 4. Rank averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "6ec40259b4636edd2c336583c5eb5c62feaaaa31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank averaging on 5 fold predictions\n"
     ]
    }
   ],
   "source": [
    "print(\"Rank averaging on\", nfold, \"fold predictions\")\n",
    "rank_predictions = np.zeros((predictions.shape[0],1))\n",
    "for i in range(nfold):\n",
    "    rank_predictions[:, 0] = np.add(rank_predictions[:, 0], rankdata(predictions[:, i].reshape(-1,1))/rank_predictions.shape[0]) \n",
    "\n",
    "rank_predictions /= nfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74eea39312bae04f6542a2f557c0454eda19d2f3"
   },
   "source": [
    "Let's submit prediction to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08a4c881ae24d784e9ee3c197d1188e26fdb40c7"
   },
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "## 5. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "1fe8a68970387c8e57d373b4d0944731a32ccd51"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.693134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0.875524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.889322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.881537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.539898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_5</td>\n",
       "      <td>0.012080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_6</td>\n",
       "      <td>0.120680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_7</td>\n",
       "      <td>0.849158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_8</td>\n",
       "      <td>0.032887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test_9</td>\n",
       "      <td>0.125836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    target\n",
       "0  test_0  0.693134\n",
       "1  test_1  0.875524\n",
       "2  test_2  0.889322\n",
       "3  test_3  0.881537\n",
       "4  test_4  0.539898\n",
       "5  test_5  0.012080\n",
       "6  test_6  0.120680\n",
       "7  test_7  0.849158\n",
       "8  test_8  0.032887\n",
       "9  test_9  0.125836"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "sub_df[\"target\"] = rank_predictions\n",
    "sub_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "65cd74398d3f80186d65a2ff3431403a6846e229"
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"Customer_Transaction_rank_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba5573cfaec6625aed13e98c6e034809e2997b5b"
   },
   "source": [
    "Do not forget to upvote :) Also fork and modify for your own use. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "497ff211885b3ab9864adf2d2938d2d06050974d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
